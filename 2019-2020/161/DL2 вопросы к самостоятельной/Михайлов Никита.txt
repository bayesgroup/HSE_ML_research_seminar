
Вопрос 1
Что такое Top-k operator в работе Large Scale Memory with Product Keys? Как он применяется в механизме внимания? Является ли итоговый механизм внимания дифференцируемым в каждой точке?
Вопрос 2
Опишите алгоритм вычисления внимания в статье Reformer: The Efficient Transformer. За счет чего авторам удается повысить эффективность алгоритма?
Вопрос 3
Какую репараметризацию для параметров масштабирования сети предлагают авторы EfficientNet? Как число операций с плавающей точкой зависит от введенного параметра \phi (compound scale coefficient)?
Вопрос 4
Как оптимизационную задачу решали авторы EfficientNet при подборе архитектур? Каким алгоритмом оптимизации авторы пользовались при поиске решения?
Вопрос 5
Как связано вычисление приближенного апостериорного распределения на веса модели с процедурой сжатия в работе Bayesian Compession for Deep Learning? За что отвечает скрытая переменная z в модели?
Вопрос 6
Зачем нужны обратимые слои в работе Reformer: The Efficient Transformer? Запишите формулу прямого и обратного прохода по ним.