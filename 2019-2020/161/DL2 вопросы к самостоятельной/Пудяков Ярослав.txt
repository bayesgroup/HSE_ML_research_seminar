
Вопрос 1
По каким параметрам авторы EfficientNet предлагают масштабировать сеть? Как асимптотически себя ведут затраты по памяти при обучении и количество весов в модели при масштабировании этих параметров?
Вопрос 2
Как связано вычисление приближенного апостериорного распределения на веса модели с процедурой сжатия в работе Bayesian Compession for Deep Learning? За что отвечает скрытая переменная z в модели?
Вопрос 3
Как использование декартового произведения ключей позволяет ускорить поиск ближайших соседей? Какая сложность поиска в эффективном алгоритме и классическом? Почему?
Вопрос 4
Что такое Top-k operator в работе Large Scale Memory with Product Keys? Как он применяется в механизме внимания? Является ли итоговый механизм внимания дифференцируемым в каждой точке?
Вопрос 5
Какое априорное распределение у переменной z в работе Bayesian Compression for Deep Learning? Какое приближенное вариационное распределение используется для этой переменной? Какие трюки предлагают авторы для снижения дисперсии при приближении функции потерь?
Вопрос 6
Опишите алгоритм вычисления внимания в статье Reformer: The Efficient Transformer. За счет чего авторам удается повысить эффективность алгоритма?