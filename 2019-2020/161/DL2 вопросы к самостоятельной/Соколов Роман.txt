
Вопрос 1
Как оптимизационную задачу решали авторы EfficientNet при подборе архитектур? Каким алгоритмом оптимизации авторы пользовались при поиске решения?
Вопрос 2
По каким параметрам авторы EfficientNet предлагают масштабировать сеть? Как асимптотически себя ведут затраты по памяти при обучении и количество весов в модели при масштабировании этих параметров?
Вопрос 3
Зачем нужны обратимые слои в работе Reformer: The Efficient Transformer? Запишите формулу прямого и обратного прохода по ним.
Вопрос 4
Какое априорное распределение у переменной z в работе Bayesian Compression for Deep Learning? Какое приближенное вариационное распределение используется для этой переменной? Какие трюки предлагают авторы для снижения дисперсии при приближении функции потерь?
Вопрос 5
Опишите алгоритм вычисления внимания в статье Reformer: The Efficient Transformer. За счет чего авторам удается повысить эффективность алгоритма?
Вопрос 6
Как авторы Large Scale Memory with Product Keys предлагают использовать слой с внешней памятью в архитектуре трансформера?