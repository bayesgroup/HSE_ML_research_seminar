{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thinking Like Transformers - code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ylz4MVbdAoB"
      },
      "source": [
        "# Thinking Like Transformers\n",
        "\n",
        "[Paper on arXiv](https://arxiv.org/abs/2106.06981)\n",
        "\n",
        "Notebook author: Polina Guseva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuTQIDXMc5ZA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKPeB5Itq7jg"
      },
      "source": [
        "## Training setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIIbUY2i96yS"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0., max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.pe[:x.size(0), :]\n",
        "        x = x + embedding\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.0, max_len=512):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.d_forwards = d_model\n",
        "        self.forwards_embedding = nn.Embedding(max_len, self.d_forwards)\n",
        "\n",
        "    def forward(self, x, real_lengths=None):\n",
        "        seq_len = x.shape[0]\n",
        "        def make_indices_tensor(indices):\n",
        "            res = torch.LongTensor([indices]).transpose(0,1)\n",
        "            if next(self.parameters()).is_cuda:  res = res.cuda()\n",
        "            return res\n",
        "        x[:,:,:self.d_forwards]+= self.forwards_embedding(make_indices_tensor(list(range(seq_len))))\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class FullEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_tokens, max_len,\n",
        "                       positional_encoding_type, positional_dropout):\n",
        "        super(FullEmbedding,self).__init__()\n",
        "\n",
        "        position_modules = {\"sin\":PositionalEncoding,\n",
        "                            \"embed\":PositionalEmbedding}\n",
        "        position_module = position_modules[positional_encoding_type]\n",
        "\n",
        "        positional_encoding = position_module(d_model,positional_dropout,\n",
        "            max_len=max_len)\n",
        "        \n",
        "        word_embedding = nn.Embedding(num_tokens, d_model)\n",
        "\n",
        "        self.word = word_embedding\n",
        "        self.pos = positional_encoding\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self,x,real_lengths=None):\n",
        "        res = self.word(x)\n",
        "        return self.pos(res)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OyVWJgG1-4N"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, heads, layers, dim_feedforward, out_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = FullEmbedding(d_model, vocab_size, 100, \"sin\", 0.)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, heads, dim_feedforward, activation=\"relu\", batch_first=True, dropout=0.\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, layers)\n",
        "\n",
        "        self.projection = nn.Linear(d_model, out_size)\n",
        "    \n",
        "        self.is_classifier = out_size > 1\n",
        "    \n",
        "    def task_type(self):\n",
        "        if self.is_classifier:\n",
        "            return \"classification\"\n",
        "        return \"regression\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        out = self.encoder(x)\n",
        "        out = self.projection(out)\n",
        "        if not self.is_classifier:\n",
        "            out = out.squeeze(-1)\n",
        "        return out"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCZhyn8LDp3x"
      },
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, optimizer, device,\n",
        "                 train_dataloader, val_dataloader, scheduler, **kwargs):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        self.cur_epoch = 1\n",
        "        self.num_epochs = kwargs.get(\"num_epochs\", 10)\n",
        "\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "\n",
        "        self.max_gradient_norm = kwargs.get(\"max_gradient_norm\", 3.0)\n",
        "\n",
        "        self.log_step = 100\n",
        "\n",
        "        self.device = device\n",
        "    \n",
        "    @staticmethod\n",
        "    def move_to_device(batch, device):\n",
        "        for key in [\"src\", \"tgt\"]:\n",
        "            batch[key] = batch[key].to(device)\n",
        "        return batch\n",
        "\n",
        "    def train(self):\n",
        "        start_epoch = self.cur_epoch\n",
        "        for epoch in range(start_epoch, self.num_epochs + 1):\n",
        "            self._train_epoch(epoch)\n",
        "            self.cur_epoch += 1\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "    \n",
        "    def process_batch(self, batch, is_training):\n",
        "        batch = self.move_to_device(batch, self.device)\n",
        "    \n",
        "        outputs = self.model(batch[\"src\"])\n",
        "        \n",
        "        if self.model.task_type() == \"classification\":\n",
        "            loss = F.cross_entropy(outputs.transpose(1, 2), batch[\"tgt\"])\n",
        "\n",
        "        else:\n",
        "            loss = F.mse_loss(outputs, batch[\"tgt\"].float())\n",
        "        \n",
        "        if is_training:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), self.max_gradient_norm)\n",
        "            self.optimizer.step()\n",
        "\n",
        "        preds = outputs.detach()\n",
        "        if self.model.task_type() == \"classification\":\n",
        "            preds = preds.argmax(dim=-1)\n",
        "        else:\n",
        "            preds = torch.round(preds)\n",
        "\n",
        "        return loss, preds\n",
        "\n",
        "    def _train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        for batch_idx, batch in enumerate(self.train_dataloader):\n",
        "            self.optimizer.zero_grad()\n",
        "            loss, preds = self.process_batch(batch, is_training=True)\n",
        "\n",
        "            if batch_idx % self.log_step == 0:\n",
        "                acc = (preds == batch[\"tgt\"]).float().mean()\n",
        "                print(\n",
        "                    \"TRAIN (epoch {}, batch {}) - accuracy {:.4f} - loss {:.4f} - grad norm {:.4f}\".format(\n",
        "                        epoch, batch_idx, acc, loss, self.get_grad_norm()\n",
        "                ))\n",
        "        \n",
        "        self._valid_epoch(epoch)\n",
        "\n",
        "    def _valid_epoch(self, epoch):\n",
        "        self.model.eval()\n",
        "        sum_correct = 0\n",
        "        sum_loss = 0.\n",
        "        num_datapoints = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(self.val_dataloader):\n",
        "                loss, preds = self.process_batch(batch, is_training=False)\n",
        "\n",
        "                batch_size = preds.size(0) * preds.size(1)\n",
        "                sum_correct += (preds == batch[\"tgt\"]).float().sum()\n",
        "                sum_loss += loss * batch_size\n",
        "                num_datapoints += batch_size\n",
        "            \n",
        "            print(\"VALID (epoch {}) - accuracy {:.4f} - loss {:.4f}\".format(\n",
        "                epoch, sum_correct / num_datapoints, sum_loss / num_datapoints\n",
        "            ))\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def get_grad_norm(self):\n",
        "        params = [p for p in self.model.parameters() if p.grad is not None]\n",
        "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), p=2).cpu() for p in params]), p=2)\n",
        "        return total_norm.item()"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AkBrZJA2JNS"
      },
      "source": [
        "## Replication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9ScpAC6Rln9"
      },
      "source": [
        "For simplicity, we are going to work with sequences that have the same fixed length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nN_WS-A81cF"
      },
      "source": [
        "SEED = 42"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdpHcmuTUfuG"
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    random.seed(seed)\n",
        "    \n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBJVRUNRRfth"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TmoFp2_U6AK"
      },
      "source": [
        "import string"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYZcmIJERdIF"
      },
      "source": [
        "def generate_data(rng, dataset_size, alphabet_size, seq_len, out_filepath):\n",
        "    assert 0 < alphabet_size <= 26\n",
        "\n",
        "    alphabet = list(string.ascii_lowercase[:alphabet_size])\n",
        "    with open(out_filepath, \"w\") as out_file:\n",
        "        for _ in range(dataset_size):\n",
        "            seq = rng.choice(alphabet, size=seq_len)\n",
        "            seq = \"\".join(seq)\n",
        "            out_file.write(seq)\n",
        "            out_file.write(\"\\n\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfemndCZWiab"
      },
      "source": [
        "TRAIN_SIZE = 10_000\n",
        "VAL_SIZE = 1_000\n",
        "TEST_SIZE = 1_000\n",
        "\n",
        "ALPHABET_SIZE = 8\n",
        "SEQ_LEN = 20\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "generate_data(rng, TRAIN_SIZE, ALPHABET_SIZE, SEQ_LEN, f\"train_a{ALPHABET_SIZE}_seq{SEQ_LEN}.txt\")\n",
        "generate_data(rng, VAL_SIZE, ALPHABET_SIZE, SEQ_LEN, f\"val_a{ALPHABET_SIZE}_seq{SEQ_LEN}.txt\")\n",
        "generate_data(rng, TEST_SIZE, ALPHABET_SIZE, SEQ_LEN, f\"test_a{ALPHABET_SIZE}_seq{SEQ_LEN}.txt\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlLkMjBnZ-Rp"
      },
      "source": [
        "import linecache"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH6r0QRVXsu5"
      },
      "source": [
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, datafile, tgt_transform, bos_id=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.datafile = datafile\n",
        "        self.tgt_transform = tgt_transform\n",
        "\n",
        "        self.bos_id = bos_id\n",
        "\n",
        "        self.dataset_size = 0\n",
        "        with open(self.datafile) as f:\n",
        "            for _ in f:\n",
        "                self.dataset_size += 1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src = linecache.getline(self.datafile, idx + 1).strip()\n",
        "        src = list(map(lambda x: string.ascii_lowercase.index(x), src))\n",
        "\n",
        "        tgt = self.tgt_transform(src)\n",
        "\n",
        "        if self.bos_id is not None:\n",
        "            src = [self.bos_id] + src\n",
        "            tgt = [self.bos_id] + tgt + ([self.bos_id] * (len(src) - len(tgt) - 1))\n",
        "\n",
        "        return {\n",
        "            \"src\": src,\n",
        "            \"tgt\": tgt,\n",
        "        }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voa34ycMmWqc"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    result = dict()\n",
        "    for key in [\"src\", \"tgt\"]:\n",
        "        result[key] = torch.cat(\n",
        "            [torch.LongTensor(item[key]).unsqueeze(0) for item in batch], dim=0\n",
        "        )\n",
        "\n",
        "    return result"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3b-0PYlAjih"
      },
      "source": [
        "def test_eval(filepath, transform_func, model, device):\n",
        "    dataset = SequenceDataset(filepath, transform_func)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    model.eval()\n",
        "    sum_correct = 0\n",
        "    sum_loss = 0\n",
        "    num_datapoints = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            batch = Trainer.move_to_device(batch, device)\n",
        "\n",
        "            outputs = model(batch[\"src\"])\n",
        "\n",
        "            if model.task_type() == \"classification\":\n",
        "                loss = F.cross_entropy(outputs.transpose(1, 2), batch[\"tgt\"])\n",
        "\n",
        "            else:\n",
        "                loss = F.mse_loss(outputs, batch[\"tgt\"].float())\n",
        "\n",
        "            preds = outputs.detach()\n",
        "            if model.task_type() == \"classification\":\n",
        "                preds = preds.argmax(dim=-1)\n",
        "            else:\n",
        "                preds = torch.round(preds)\n",
        "\n",
        "            batch_size = preds.size(0) * preds.size(1)\n",
        "            sum_correct += (preds == batch[\"tgt\"]).float().sum()\n",
        "            sum_loss += loss * batch_size\n",
        "            num_datapoints += batch_size\n",
        "    \n",
        "    return sum_correct / num_datapoints, sum_loss / num_datapoints"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PA0t5AbDLy0"
      },
      "source": [
        "### Tools for extracting attention maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2vZW8IDDPf3"
      },
      "source": [
        "class OutputHook:\n",
        "    def __init__(self, name):\n",
        "        self.saved_outs = []\n",
        "        self.name = name\n",
        "\n",
        "    def __call__(self, module, input, output):\n",
        "        self.saved_outs.append(output[1].detach().cpu())"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2WBkJgoDS10"
      },
      "source": [
        "def add_hooks(model):\n",
        "    hooks = dict()\n",
        "    handles = []\n",
        "    for layer_id, layer in enumerate(model.encoder.layers):\n",
        "        name = f\"l{layer_id}\"\n",
        "        hook = OutputHook(name)\n",
        "        hooks[name] = hook\n",
        "        handles.append(layer.self_attn.register_forward_hook(hook))\n",
        "\n",
        "    return hooks, handles\n",
        "\n",
        "def remove_hooks(hook_handles):\n",
        "    for handle in hook_handles:\n",
        "        handle.remove()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_KCkZ0UjMoj"
      },
      "source": [
        "### Histograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlnZ0qdGkyKg"
      },
      "source": [
        "set_seed(SEED)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3ULOGbtfNIK"
      },
      "source": [
        "model = Encoder(ALPHABET_SIZE, 256, 1, 1, 512, 1)\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
        "scheduler = None"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwdYBb1SnAvQ"
      },
      "source": [
        "def hist(x):\n",
        "    _, inv, cnt = np.unique(x, return_inverse=True, return_counts=True)\n",
        "    return list(cnt[inv])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebhEFFYmkPts"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = SequenceDataset(\"train_a8_seq20.txt\", hist)\n",
        "val_dataset = SequenceDataset(\"val_a8_seq20.txt\", hist)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, collate_fn=collate_fn)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAhPQx3alGd4"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1Fc8IOklUyF",
        "outputId": "fab95796-2fd9-45dc-b883-e826a7f9c899"
      },
      "source": [
        "model = model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model, \n",
        "    optim,\n",
        "    device, \n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    scheduler,\n",
        "    max_gradient_norm=10.,\n",
        "    num_epochs=50,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN (epoch 1, batch 0) - accuracy 0.0203 - loss 10.1641 - grad norm 10.0000\n",
            "TRAIN (epoch 1, batch 100) - accuracy 0.3531 - loss 1.5386 - grad norm 5.5502\n",
            "TRAIN (epoch 1, batch 200) - accuracy 0.4203 - loss 1.2735 - grad norm 10.0000\n",
            "TRAIN (epoch 1, batch 300) - accuracy 0.4375 - loss 1.3563 - grad norm 6.3124\n",
            "VALID (epoch 1) - accuracy 0.4691 - loss 1.0919\n",
            "TRAIN (epoch 2, batch 0) - accuracy 0.4672 - loss 0.8027 - grad norm 10.0000\n",
            "TRAIN (epoch 2, batch 100) - accuracy 0.5078 - loss 0.9968 - grad norm 10.0000\n",
            "TRAIN (epoch 2, batch 200) - accuracy 0.4719 - loss 1.0750 - grad norm 9.4165\n",
            "TRAIN (epoch 2, batch 300) - accuracy 0.4437 - loss 1.2627 - grad norm 6.0833\n",
            "VALID (epoch 2) - accuracy 0.5237 - loss 1.0111\n",
            "TRAIN (epoch 3, batch 0) - accuracy 0.5375 - loss 0.7004 - grad norm 8.0226\n",
            "TRAIN (epoch 3, batch 100) - accuracy 0.5203 - loss 0.9320 - grad norm 8.6834\n",
            "TRAIN (epoch 3, batch 200) - accuracy 0.4719 - loss 1.0584 - grad norm 8.5109\n",
            "TRAIN (epoch 3, batch 300) - accuracy 0.4437 - loss 1.2510 - grad norm 5.7596\n",
            "VALID (epoch 3) - accuracy 0.5189 - loss 1.0191\n",
            "TRAIN (epoch 4, batch 0) - accuracy 0.5594 - loss 0.6928 - grad norm 6.3619\n",
            "TRAIN (epoch 4, batch 100) - accuracy 0.5297 - loss 0.9108 - grad norm 7.4399\n",
            "TRAIN (epoch 4, batch 200) - accuracy 0.4813 - loss 1.0432 - grad norm 7.8217\n",
            "TRAIN (epoch 4, batch 300) - accuracy 0.4531 - loss 1.2425 - grad norm 5.6076\n",
            "VALID (epoch 4) - accuracy 0.5224 - loss 1.0078\n",
            "TRAIN (epoch 5, batch 0) - accuracy 0.5797 - loss 0.6983 - grad norm 5.9410\n",
            "TRAIN (epoch 5, batch 100) - accuracy 0.5375 - loss 0.9166 - grad norm 7.9852\n",
            "TRAIN (epoch 5, batch 200) - accuracy 0.4828 - loss 1.0363 - grad norm 7.3208\n",
            "TRAIN (epoch 5, batch 300) - accuracy 0.4547 - loss 1.2382 - grad norm 5.8282\n",
            "VALID (epoch 5) - accuracy 0.5213 - loss 1.0058\n",
            "TRAIN (epoch 6, batch 0) - accuracy 0.5672 - loss 0.7007 - grad norm 5.4180\n",
            "TRAIN (epoch 6, batch 100) - accuracy 0.5406 - loss 0.9265 - grad norm 8.5005\n",
            "TRAIN (epoch 6, batch 200) - accuracy 0.4797 - loss 1.0386 - grad norm 7.1632\n",
            "TRAIN (epoch 6, batch 300) - accuracy 0.4437 - loss 1.2278 - grad norm 5.6312\n",
            "VALID (epoch 6) - accuracy 0.5282 - loss 0.9865\n",
            "TRAIN (epoch 7, batch 0) - accuracy 0.5437 - loss 0.7041 - grad norm 5.3309\n",
            "TRAIN (epoch 7, batch 100) - accuracy 0.5344 - loss 0.9367 - grad norm 8.9274\n",
            "TRAIN (epoch 7, batch 200) - accuracy 0.4766 - loss 1.0290 - grad norm 6.5604\n",
            "TRAIN (epoch 7, batch 300) - accuracy 0.4547 - loss 1.2159 - grad norm 5.2603\n",
            "VALID (epoch 7) - accuracy 0.5360 - loss 0.9807\n",
            "TRAIN (epoch 8, batch 0) - accuracy 0.5484 - loss 0.7063 - grad norm 5.2926\n",
            "TRAIN (epoch 8, batch 100) - accuracy 0.5266 - loss 0.9421 - grad norm 8.9189\n",
            "TRAIN (epoch 8, batch 200) - accuracy 0.4813 - loss 1.0132 - grad norm 6.0503\n",
            "TRAIN (epoch 8, batch 300) - accuracy 0.4563 - loss 1.1993 - grad norm 4.8310\n",
            "VALID (epoch 8) - accuracy 0.5429 - loss 0.9692\n",
            "TRAIN (epoch 9, batch 0) - accuracy 0.5563 - loss 0.6943 - grad norm 5.1921\n",
            "TRAIN (epoch 9, batch 100) - accuracy 0.5594 - loss 0.9041 - grad norm 8.0337\n",
            "TRAIN (epoch 9, batch 200) - accuracy 0.5516 - loss 0.9366 - grad norm 5.2059\n",
            "TRAIN (epoch 9, batch 300) - accuracy 0.5125 - loss 1.1278 - grad norm 3.8498\n",
            "VALID (epoch 9) - accuracy 0.5835 - loss 0.9296\n",
            "TRAIN (epoch 10, batch 0) - accuracy 0.6328 - loss 0.6430 - grad norm 4.4291\n",
            "TRAIN (epoch 10, batch 100) - accuracy 0.5891 - loss 0.8617 - grad norm 7.4508\n",
            "TRAIN (epoch 10, batch 200) - accuracy 0.6078 - loss 0.8852 - grad norm 4.6672\n",
            "TRAIN (epoch 10, batch 300) - accuracy 0.5859 - loss 1.0087 - grad norm 3.9338\n",
            "VALID (epoch 10) - accuracy 0.6481 - loss 0.8721\n",
            "TRAIN (epoch 11, batch 0) - accuracy 0.6953 - loss 0.5974 - grad norm 4.7490\n",
            "TRAIN (epoch 11, batch 100) - accuracy 0.6516 - loss 0.8027 - grad norm 5.8524\n",
            "TRAIN (epoch 11, batch 200) - accuracy 0.6969 - loss 0.7094 - grad norm 5.7132\n",
            "TRAIN (epoch 11, batch 300) - accuracy 0.7812 - loss 0.5366 - grad norm 2.8732\n",
            "VALID (epoch 11) - accuracy 0.7724 - loss 0.6163\n",
            "TRAIN (epoch 12, batch 0) - accuracy 0.7859 - loss 0.3988 - grad norm 5.1162\n",
            "TRAIN (epoch 12, batch 100) - accuracy 0.8109 - loss 0.3445 - grad norm 1.2475\n",
            "TRAIN (epoch 12, batch 200) - accuracy 0.7406 - loss 0.6871 - grad norm 5.8584\n",
            "TRAIN (epoch 12, batch 300) - accuracy 0.7969 - loss 0.5230 - grad norm 3.6219\n",
            "VALID (epoch 12) - accuracy 0.7741 - loss 0.6078\n",
            "TRAIN (epoch 13, batch 0) - accuracy 0.7937 - loss 0.3925 - grad norm 4.5226\n",
            "TRAIN (epoch 13, batch 100) - accuracy 0.8344 - loss 0.3421 - grad norm 1.6955\n",
            "TRAIN (epoch 13, batch 200) - accuracy 0.7406 - loss 0.6886 - grad norm 5.7555\n",
            "TRAIN (epoch 13, batch 300) - accuracy 0.8031 - loss 0.5139 - grad norm 2.9674\n",
            "VALID (epoch 13) - accuracy 0.7752 - loss 0.6066\n",
            "TRAIN (epoch 14, batch 0) - accuracy 0.7937 - loss 0.3870 - grad norm 3.9821\n",
            "TRAIN (epoch 14, batch 100) - accuracy 0.8219 - loss 0.3423 - grad norm 1.9218\n",
            "TRAIN (epoch 14, batch 200) - accuracy 0.7406 - loss 0.6929 - grad norm 5.7638\n",
            "TRAIN (epoch 14, batch 300) - accuracy 0.8109 - loss 0.5104 - grad norm 2.5811\n",
            "VALID (epoch 14) - accuracy 0.7796 - loss 0.6021\n",
            "TRAIN (epoch 15, batch 0) - accuracy 0.7937 - loss 0.3821 - grad norm 3.2947\n",
            "TRAIN (epoch 15, batch 100) - accuracy 0.8219 - loss 0.3406 - grad norm 1.9002\n",
            "TRAIN (epoch 15, batch 200) - accuracy 0.7406 - loss 0.6902 - grad norm 5.5595\n",
            "TRAIN (epoch 15, batch 300) - accuracy 0.8125 - loss 0.5051 - grad norm 2.3250\n",
            "VALID (epoch 15) - accuracy 0.7873 - loss 0.6003\n",
            "TRAIN (epoch 16, batch 0) - accuracy 0.8125 - loss 0.3776 - grad norm 2.7268\n",
            "TRAIN (epoch 16, batch 100) - accuracy 0.8219 - loss 0.3394 - grad norm 1.7479\n",
            "TRAIN (epoch 16, batch 200) - accuracy 0.7625 - loss 0.6739 - grad norm 5.0981\n",
            "TRAIN (epoch 16, batch 300) - accuracy 0.8266 - loss 0.4877 - grad norm 2.4146\n",
            "VALID (epoch 16) - accuracy 0.8039 - loss 0.5838\n",
            "TRAIN (epoch 17, batch 0) - accuracy 0.8250 - loss 0.3639 - grad norm 2.1525\n",
            "TRAIN (epoch 17, batch 100) - accuracy 0.8469 - loss 0.3234 - grad norm 1.0401\n",
            "TRAIN (epoch 17, batch 200) - accuracy 0.7844 - loss 0.6572 - grad norm 5.0447\n",
            "TRAIN (epoch 17, batch 300) - accuracy 0.8391 - loss 0.4813 - grad norm 2.2213\n",
            "VALID (epoch 17) - accuracy 0.8135 - loss 0.5711\n",
            "TRAIN (epoch 18, batch 0) - accuracy 0.8313 - loss 0.3581 - grad norm 1.7478\n",
            "TRAIN (epoch 18, batch 100) - accuracy 0.8469 - loss 0.3181 - grad norm 0.8482\n",
            "TRAIN (epoch 18, batch 200) - accuracy 0.7734 - loss 0.6597 - grad norm 5.0864\n",
            "TRAIN (epoch 18, batch 300) - accuracy 0.8344 - loss 0.4819 - grad norm 2.3855\n",
            "VALID (epoch 18) - accuracy 0.8142 - loss 0.5738\n",
            "TRAIN (epoch 19, batch 0) - accuracy 0.8313 - loss 0.3553 - grad norm 1.6323\n",
            "TRAIN (epoch 19, batch 100) - accuracy 0.8469 - loss 0.3162 - grad norm 0.7734\n",
            "TRAIN (epoch 19, batch 200) - accuracy 0.7781 - loss 0.6624 - grad norm 5.1538\n",
            "TRAIN (epoch 19, batch 300) - accuracy 0.8344 - loss 0.4830 - grad norm 2.2630\n",
            "VALID (epoch 19) - accuracy 0.8144 - loss 0.5710\n",
            "TRAIN (epoch 20, batch 0) - accuracy 0.8250 - loss 0.3555 - grad norm 1.7612\n",
            "TRAIN (epoch 20, batch 100) - accuracy 0.8406 - loss 0.3149 - grad norm 0.7672\n",
            "TRAIN (epoch 20, batch 200) - accuracy 0.7781 - loss 0.6618 - grad norm 5.2117\n",
            "TRAIN (epoch 20, batch 300) - accuracy 0.8344 - loss 0.4823 - grad norm 2.2257\n",
            "VALID (epoch 20) - accuracy 0.8155 - loss 0.5744\n",
            "TRAIN (epoch 21, batch 0) - accuracy 0.8250 - loss 0.3550 - grad norm 1.8356\n",
            "TRAIN (epoch 21, batch 100) - accuracy 0.8406 - loss 0.3143 - grad norm 1.0067\n",
            "TRAIN (epoch 21, batch 200) - accuracy 0.7781 - loss 0.6641 - grad norm 5.3464\n",
            "TRAIN (epoch 21, batch 300) - accuracy 0.8344 - loss 0.4816 - grad norm 2.0932\n",
            "VALID (epoch 21) - accuracy 0.8150 - loss 0.5737\n",
            "TRAIN (epoch 22, batch 0) - accuracy 0.8250 - loss 0.3530 - grad norm 1.6614\n",
            "TRAIN (epoch 22, batch 100) - accuracy 0.8406 - loss 0.3184 - grad norm 2.0554\n",
            "TRAIN (epoch 22, batch 200) - accuracy 0.7781 - loss 0.6683 - grad norm 5.5630\n",
            "TRAIN (epoch 22, batch 300) - accuracy 0.8344 - loss 0.4807 - grad norm 1.8236\n",
            "VALID (epoch 22) - accuracy 0.8150 - loss 0.5733\n",
            "TRAIN (epoch 23, batch 0) - accuracy 0.8250 - loss 0.3527 - grad norm 1.6714\n",
            "TRAIN (epoch 23, batch 100) - accuracy 0.8406 - loss 0.3172 - grad norm 2.1652\n",
            "TRAIN (epoch 23, batch 200) - accuracy 0.7781 - loss 0.6693 - grad norm 5.4941\n",
            "TRAIN (epoch 23, batch 300) - accuracy 0.8297 - loss 0.4792 - grad norm 1.9074\n",
            "VALID (epoch 23) - accuracy 0.8134 - loss 0.5753\n",
            "TRAIN (epoch 24, batch 0) - accuracy 0.8250 - loss 0.3544 - grad norm 1.8776\n",
            "TRAIN (epoch 24, batch 100) - accuracy 0.8406 - loss 0.3124 - grad norm 1.3994\n",
            "TRAIN (epoch 24, batch 200) - accuracy 0.7781 - loss 0.6717 - grad norm 5.4842\n",
            "TRAIN (epoch 24, batch 300) - accuracy 0.8344 - loss 0.4817 - grad norm 2.0178\n",
            "VALID (epoch 24) - accuracy 0.8152 - loss 0.5772\n",
            "TRAIN (epoch 25, batch 0) - accuracy 0.8313 - loss 0.3528 - grad norm 1.8711\n",
            "TRAIN (epoch 25, batch 100) - accuracy 0.8406 - loss 0.3114 - grad norm 1.4034\n",
            "TRAIN (epoch 25, batch 200) - accuracy 0.7781 - loss 0.6717 - grad norm 5.4544\n",
            "TRAIN (epoch 25, batch 300) - accuracy 0.8297 - loss 0.4829 - grad norm 1.9522\n",
            "VALID (epoch 25) - accuracy 0.8147 - loss 0.5755\n",
            "TRAIN (epoch 26, batch 0) - accuracy 0.8313 - loss 0.3540 - grad norm 1.8746\n",
            "TRAIN (epoch 26, batch 100) - accuracy 0.8406 - loss 0.3084 - grad norm 1.2432\n",
            "TRAIN (epoch 26, batch 200) - accuracy 0.7734 - loss 0.6677 - grad norm 5.3292\n",
            "TRAIN (epoch 26, batch 300) - accuracy 0.8297 - loss 0.4794 - grad norm 1.9612\n",
            "VALID (epoch 26) - accuracy 0.8155 - loss 0.5746\n",
            "TRAIN (epoch 27, batch 0) - accuracy 0.8250 - loss 0.3510 - grad norm 1.7252\n",
            "TRAIN (epoch 27, batch 100) - accuracy 0.8406 - loss 0.3051 - grad norm 1.2859\n",
            "TRAIN (epoch 27, batch 200) - accuracy 0.7734 - loss 0.6653 - grad norm 5.3121\n",
            "TRAIN (epoch 27, batch 300) - accuracy 0.8250 - loss 0.4748 - grad norm 1.8646\n",
            "VALID (epoch 27) - accuracy 0.8151 - loss 0.5716\n",
            "TRAIN (epoch 28, batch 0) - accuracy 0.8250 - loss 0.3507 - grad norm 1.5506\n",
            "TRAIN (epoch 28, batch 100) - accuracy 0.8406 - loss 0.3045 - grad norm 1.3680\n",
            "TRAIN (epoch 28, batch 200) - accuracy 0.7734 - loss 0.6574 - grad norm 5.1891\n",
            "TRAIN (epoch 28, batch 300) - accuracy 0.8203 - loss 0.4704 - grad norm 1.8711\n",
            "VALID (epoch 28) - accuracy 0.8140 - loss 0.5762\n",
            "TRAIN (epoch 29, batch 0) - accuracy 0.8297 - loss 0.3476 - grad norm 1.5202\n",
            "TRAIN (epoch 29, batch 100) - accuracy 0.8813 - loss 0.2642 - grad norm 2.8272\n",
            "TRAIN (epoch 29, batch 200) - accuracy 0.8969 - loss 0.2621 - grad norm 3.4361\n",
            "TRAIN (epoch 29, batch 300) - accuracy 0.9219 - loss 0.1788 - grad norm 1.2198\n",
            "VALID (epoch 29) - accuracy 0.9069 - loss 0.3012\n",
            "TRAIN (epoch 30, batch 0) - accuracy 0.9187 - loss 0.1943 - grad norm 1.8331\n",
            "TRAIN (epoch 30, batch 100) - accuracy 0.9203 - loss 0.2134 - grad norm 0.6328\n",
            "TRAIN (epoch 30, batch 200) - accuracy 0.8969 - loss 0.2495 - grad norm 1.7415\n",
            "TRAIN (epoch 30, batch 300) - accuracy 0.9219 - loss 0.1765 - grad norm 1.2565\n",
            "VALID (epoch 30) - accuracy 0.9063 - loss 0.3001\n",
            "TRAIN (epoch 31, batch 0) - accuracy 0.9187 - loss 0.1874 - grad norm 1.8040\n",
            "TRAIN (epoch 31, batch 100) - accuracy 0.9141 - loss 0.2133 - grad norm 0.7102\n",
            "TRAIN (epoch 31, batch 200) - accuracy 0.8969 - loss 0.2485 - grad norm 1.7303\n",
            "TRAIN (epoch 31, batch 300) - accuracy 0.9266 - loss 0.1718 - grad norm 0.7768\n",
            "VALID (epoch 31) - accuracy 0.9062 - loss 0.2982\n",
            "TRAIN (epoch 32, batch 0) - accuracy 0.9187 - loss 0.1874 - grad norm 1.9832\n",
            "TRAIN (epoch 32, batch 100) - accuracy 0.9141 - loss 0.2120 - grad norm 0.7619\n",
            "TRAIN (epoch 32, batch 200) - accuracy 0.8969 - loss 0.2486 - grad norm 1.9104\n",
            "TRAIN (epoch 32, batch 300) - accuracy 0.9219 - loss 0.1712 - grad norm 0.8354\n",
            "VALID (epoch 32) - accuracy 0.9065 - loss 0.3015\n",
            "TRAIN (epoch 33, batch 0) - accuracy 0.9187 - loss 0.1868 - grad norm 1.8904\n",
            "TRAIN (epoch 33, batch 100) - accuracy 0.9141 - loss 0.2120 - grad norm 0.9262\n",
            "TRAIN (epoch 33, batch 200) - accuracy 0.9016 - loss 0.2489 - grad norm 1.9511\n",
            "TRAIN (epoch 33, batch 300) - accuracy 0.9266 - loss 0.1704 - grad norm 0.7579\n",
            "VALID (epoch 33) - accuracy 0.9068 - loss 0.3001\n",
            "TRAIN (epoch 34, batch 0) - accuracy 0.9187 - loss 0.1843 - grad norm 1.7684\n",
            "TRAIN (epoch 34, batch 100) - accuracy 0.9141 - loss 0.2125 - grad norm 1.0699\n",
            "TRAIN (epoch 34, batch 200) - accuracy 0.9016 - loss 0.2486 - grad norm 2.0309\n",
            "TRAIN (epoch 34, batch 300) - accuracy 0.9266 - loss 0.1685 - grad norm 0.8177\n",
            "VALID (epoch 34) - accuracy 0.9077 - loss 0.3001\n",
            "TRAIN (epoch 35, batch 0) - accuracy 0.9187 - loss 0.1798 - grad norm 1.7594\n",
            "TRAIN (epoch 35, batch 100) - accuracy 0.9156 - loss 0.1232 - grad norm 3.1724\n",
            "TRAIN (epoch 35, batch 200) - accuracy 0.9875 - loss 0.0185 - grad norm 1.7167\n",
            "TRAIN (epoch 35, batch 300) - accuracy 1.0000 - loss 0.0042 - grad norm 0.7984\n",
            "VALID (epoch 35) - accuracy 0.9979 - loss 0.0060\n",
            "TRAIN (epoch 36, batch 0) - accuracy 1.0000 - loss 0.0038 - grad norm 0.9112\n",
            "TRAIN (epoch 36, batch 100) - accuracy 0.9984 - loss 0.0064 - grad norm 1.3079\n",
            "TRAIN (epoch 36, batch 200) - accuracy 1.0000 - loss 0.0033 - grad norm 0.8757\n",
            "TRAIN (epoch 36, batch 300) - accuracy 1.0000 - loss 0.0028 - grad norm 0.6285\n",
            "VALID (epoch 36) - accuracy 0.9995 - loss 0.0034\n",
            "TRAIN (epoch 37, batch 0) - accuracy 1.0000 - loss 0.0024 - grad norm 0.7622\n",
            "TRAIN (epoch 37, batch 100) - accuracy 1.0000 - loss 0.0035 - grad norm 0.9403\n",
            "TRAIN (epoch 37, batch 200) - accuracy 1.0000 - loss 0.0021 - grad norm 0.6446\n",
            "TRAIN (epoch 37, batch 300) - accuracy 1.0000 - loss 0.0035 - grad norm 0.9749\n",
            "VALID (epoch 37) - accuracy 0.9995 - loss 0.0023\n",
            "TRAIN (epoch 38, batch 0) - accuracy 1.0000 - loss 0.0017 - grad norm 0.5398\n",
            "TRAIN (epoch 38, batch 100) - accuracy 1.0000 - loss 0.0023 - grad norm 0.6731\n",
            "TRAIN (epoch 38, batch 200) - accuracy 1.0000 - loss 0.0019 - grad norm 0.5751\n",
            "TRAIN (epoch 38, batch 300) - accuracy 1.0000 - loss 0.0055 - grad norm 1.4840\n",
            "VALID (epoch 38) - accuracy 1.0000 - loss 0.0021\n",
            "TRAIN (epoch 39, batch 0) - accuracy 1.0000 - loss 0.0021 - grad norm 0.7614\n",
            "TRAIN (epoch 39, batch 100) - accuracy 1.0000 - loss 0.0031 - grad norm 0.8819\n",
            "TRAIN (epoch 39, batch 200) - accuracy 1.0000 - loss 0.0013 - grad norm 0.5638\n",
            "TRAIN (epoch 39, batch 300) - accuracy 1.0000 - loss 0.0056 - grad norm 1.4408\n",
            "VALID (epoch 39) - accuracy 1.0000 - loss 0.0029\n",
            "TRAIN (epoch 40, batch 0) - accuracy 1.0000 - loss 0.0028 - grad norm 0.9223\n",
            "TRAIN (epoch 40, batch 100) - accuracy 1.0000 - loss 0.0026 - grad norm 0.7606\n",
            "TRAIN (epoch 40, batch 200) - accuracy 1.0000 - loss 0.0008 - grad norm 0.3794\n",
            "TRAIN (epoch 40, batch 300) - accuracy 1.0000 - loss 0.0046 - grad norm 1.1929\n",
            "VALID (epoch 40) - accuracy 1.0000 - loss 0.0021\n",
            "TRAIN (epoch 41, batch 0) - accuracy 1.0000 - loss 0.0017 - grad norm 0.5565\n",
            "TRAIN (epoch 41, batch 100) - accuracy 1.0000 - loss 0.0019 - grad norm 0.5923\n",
            "TRAIN (epoch 41, batch 200) - accuracy 1.0000 - loss 0.0011 - grad norm 0.4214\n",
            "TRAIN (epoch 41, batch 300) - accuracy 1.0000 - loss 0.0037 - grad norm 0.9250\n",
            "VALID (epoch 41) - accuracy 1.0000 - loss 0.0024\n",
            "TRAIN (epoch 42, batch 0) - accuracy 1.0000 - loss 0.0019 - grad norm 0.5821\n",
            "TRAIN (epoch 42, batch 100) - accuracy 1.0000 - loss 0.0020 - grad norm 0.6063\n",
            "TRAIN (epoch 42, batch 200) - accuracy 1.0000 - loss 0.0014 - grad norm 0.6652\n",
            "TRAIN (epoch 42, batch 300) - accuracy 1.0000 - loss 0.0025 - grad norm 0.6641\n",
            "VALID (epoch 42) - accuracy 1.0000 - loss 0.0030\n",
            "TRAIN (epoch 43, batch 0) - accuracy 1.0000 - loss 0.0026 - grad norm 0.7862\n",
            "TRAIN (epoch 43, batch 100) - accuracy 1.0000 - loss 0.0018 - grad norm 0.6581\n",
            "TRAIN (epoch 43, batch 200) - accuracy 1.0000 - loss 0.0015 - grad norm 0.6359\n",
            "TRAIN (epoch 43, batch 300) - accuracy 1.0000 - loss 0.0027 - grad norm 0.8356\n",
            "VALID (epoch 43) - accuracy 1.0000 - loss 0.0060\n",
            "TRAIN (epoch 44, batch 0) - accuracy 1.0000 - loss 0.0053 - grad norm 1.2974\n",
            "TRAIN (epoch 44, batch 100) - accuracy 1.0000 - loss 0.0012 - grad norm 0.4904\n",
            "TRAIN (epoch 44, batch 200) - accuracy 1.0000 - loss 0.0030 - grad norm 0.8337\n",
            "TRAIN (epoch 44, batch 300) - accuracy 1.0000 - loss 0.0018 - grad norm 0.4923\n",
            "VALID (epoch 44) - accuracy 1.0000 - loss 0.0012\n",
            "TRAIN (epoch 45, batch 0) - accuracy 1.0000 - loss 0.0008 - grad norm 0.3085\n",
            "TRAIN (epoch 45, batch 100) - accuracy 1.0000 - loss 0.0020 - grad norm 0.6495\n",
            "TRAIN (epoch 45, batch 200) - accuracy 1.0000 - loss 0.0017 - grad norm 0.6260\n",
            "TRAIN (epoch 45, batch 300) - accuracy 1.0000 - loss 0.0017 - grad norm 0.6044\n",
            "VALID (epoch 45) - accuracy 1.0000 - loss 0.0024\n",
            "TRAIN (epoch 46, batch 0) - accuracy 1.0000 - loss 0.0018 - grad norm 0.7080\n",
            "TRAIN (epoch 46, batch 100) - accuracy 1.0000 - loss 0.0022 - grad norm 0.7645\n",
            "TRAIN (epoch 46, batch 200) - accuracy 1.0000 - loss 0.0021 - grad norm 0.8072\n",
            "TRAIN (epoch 46, batch 300) - accuracy 1.0000 - loss 0.0012 - grad norm 0.4020\n",
            "VALID (epoch 46) - accuracy 1.0000 - loss 0.0009\n",
            "TRAIN (epoch 47, batch 0) - accuracy 1.0000 - loss 0.0007 - grad norm 0.3232\n",
            "TRAIN (epoch 47, batch 100) - accuracy 1.0000 - loss 0.0015 - grad norm 0.5371\n",
            "TRAIN (epoch 47, batch 200) - accuracy 1.0000 - loss 0.0009 - grad norm 0.3784\n",
            "TRAIN (epoch 47, batch 300) - accuracy 1.0000 - loss 0.0012 - grad norm 0.4649\n",
            "VALID (epoch 47) - accuracy 1.0000 - loss 0.0020\n",
            "TRAIN (epoch 48, batch 0) - accuracy 1.0000 - loss 0.0016 - grad norm 0.6371\n",
            "TRAIN (epoch 48, batch 100) - accuracy 1.0000 - loss 0.0017 - grad norm 0.6050\n",
            "TRAIN (epoch 48, batch 200) - accuracy 1.0000 - loss 0.0010 - grad norm 0.4472\n",
            "TRAIN (epoch 48, batch 300) - accuracy 1.0000 - loss 0.0018 - grad norm 0.4415\n",
            "VALID (epoch 48) - accuracy 1.0000 - loss 0.0019\n",
            "TRAIN (epoch 49, batch 0) - accuracy 1.0000 - loss 0.0015 - grad norm 0.5231\n",
            "TRAIN (epoch 49, batch 100) - accuracy 1.0000 - loss 0.0052 - grad norm 1.1291\n",
            "TRAIN (epoch 49, batch 200) - accuracy 1.0000 - loss 0.0034 - grad norm 0.8195\n",
            "TRAIN (epoch 49, batch 300) - accuracy 1.0000 - loss 0.0017 - grad norm 0.5501\n",
            "VALID (epoch 49) - accuracy 0.9995 - loss 0.0016\n",
            "TRAIN (epoch 50, batch 0) - accuracy 1.0000 - loss 0.0013 - grad norm 0.5730\n",
            "TRAIN (epoch 50, batch 100) - accuracy 1.0000 - loss 0.0021 - grad norm 0.7221\n",
            "TRAIN (epoch 50, batch 200) - accuracy 1.0000 - loss 0.0011 - grad norm 0.4507\n",
            "TRAIN (epoch 50, batch 300) - accuracy 1.0000 - loss 0.0017 - grad norm 0.6617\n",
            "VALID (epoch 50) - accuracy 0.9995 - loss 0.0013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjNmCDeb_v1E"
      },
      "source": [
        "Paper states that on Hist language upper bound is (1, 2). With 1 layer and 2 heads transformer is able to reach accuracy 99.9, but with 1 layer and 1 head one can only reach accuracy of 91.9.\n",
        "\n",
        "The experiment shows that transformer with L=1, H=1 is capable of learning Hist language, hence the claim in the paper is false. It is possible that hyperparameters in paper aren't well optimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n0xU687jDr6n",
        "outputId": "47b7cb7b-3851-4828-ee0b-e245d5a05043"
      },
      "source": [
        "acc, loss = test_eval(\"test_a8_seq20.txt\", hist, model, device)\n",
        "\n",
        "\"TEST acc {:.4f} - loss {:.4f}\".format(acc, loss)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TEST acc 1.0000 - loss 0.0013'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZk9IheyJSqn"
      },
      "source": [
        "Also let's look at attention map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEjQ60C0CStT"
      },
      "source": [
        "hooks, handles = add_hooks(model)\n",
        "\n",
        "seq = np.random.choice(np.arange(ALPHABET_SIZE), size=SEQ_LEN)\n",
        "\n",
        "model(torch.LongTensor([seq]).to(\"cuda\"))\n",
        "\n",
        "remove_hooks(handles)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7dUz5vGJnzp"
      },
      "source": [
        "attention_map = hooks[\"l0\"].saved_outs[0].squeeze(0)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "aEk87QZRJqyo",
        "outputId": "e3e783a9-b1d0-484f-ab3d-e8823661d486"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "decoded_seq = [string.ascii_lowercase[id] for id in seq]\n",
        "\n",
        "ax = seaborn.heatmap(\n",
        "    attention_map, \n",
        "    square=True, \n",
        "    xticklabels=decoded_seq, \n",
        "    yticklabels=decoded_seq\n",
        ")\n",
        "\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMTe1T_wMzrd"
      },
      "source": [
        "### Count greater values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhHjAdTfKK_y"
      },
      "source": [
        "set_seed(SEED)"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o3J5J1LNLv1"
      },
      "source": [
        "model = Encoder(ALPHABET_SIZE, 256, 1, 1, 512, 1)\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.99)"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S8zfMUxNbq9"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def c_hist(x):\n",
        "    unique_vals, inv, cnts = np.unique(x, return_inverse=True, return_counts=True)\n",
        "    greater_cnts = len(x) - np.cumsum(cnts)\n",
        "    return list(greater_cnts[inv])"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smP8HgwRNijE"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = SequenceDataset(\"train_a8_seq20.txt\", c_hist)\n",
        "val_dataset = SequenceDataset(\"val_a8_seq20.txt\", c_hist)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, collate_fn=collate_fn)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-rOCFquOGYz"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McVAc7yPOKqC",
        "outputId": "9591d149-4a53-4644-d4b3-3ec39455747d"
      },
      "source": [
        "model = model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model, \n",
        "    optim,\n",
        "    device, \n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    scheduler,\n",
        "    max_gradient_norm=5.,\n",
        "    num_epochs=15,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN (epoch 1, batch 0) - accuracy 0.0625 - loss 98.3075 - grad norm 5.0000\n",
            "TRAIN (epoch 1, batch 100) - accuracy 0.2938 - loss 2.5228 - grad norm 5.0000\n",
            "TRAIN (epoch 1, batch 200) - accuracy 0.3609 - loss 1.2700 - grad norm 5.0000\n",
            "TRAIN (epoch 1, batch 300) - accuracy 0.5906 - loss 0.7224 - grad norm 5.0000\n",
            "VALID (epoch 1) - accuracy 0.6341 - loss 0.4140\n",
            "TRAIN (epoch 2, batch 0) - accuracy 0.7063 - loss 0.2957 - grad norm 4.3974\n",
            "TRAIN (epoch 2, batch 100) - accuracy 0.7500 - loss 0.2276 - grad norm 4.2895\n",
            "TRAIN (epoch 2, batch 200) - accuracy 0.8297 - loss 0.1664 - grad norm 4.0373\n",
            "TRAIN (epoch 2, batch 300) - accuracy 0.7266 - loss 0.2269 - grad norm 5.0000\n",
            "VALID (epoch 2) - accuracy 0.9085 - loss 0.0943\n",
            "TRAIN (epoch 3, batch 0) - accuracy 0.9375 - loss 0.0672 - grad norm 4.7017\n",
            "TRAIN (epoch 3, batch 100) - accuracy 0.9594 - loss 0.0653 - grad norm 3.7551\n",
            "TRAIN (epoch 3, batch 200) - accuracy 0.9500 - loss 0.0504 - grad norm 2.7540\n",
            "TRAIN (epoch 3, batch 300) - accuracy 0.8672 - loss 0.0966 - grad norm 3.5256\n",
            "VALID (epoch 3) - accuracy 0.9260 - loss 0.0756\n",
            "TRAIN (epoch 4, batch 0) - accuracy 0.9422 - loss 0.0656 - grad norm 2.8433\n",
            "TRAIN (epoch 4, batch 100) - accuracy 0.9438 - loss 0.0617 - grad norm 2.7103\n",
            "TRAIN (epoch 4, batch 200) - accuracy 0.9672 - loss 0.0436 - grad norm 3.1968\n",
            "TRAIN (epoch 4, batch 300) - accuracy 0.9406 - loss 0.0534 - grad norm 1.6062\n",
            "VALID (epoch 4) - accuracy 0.9579 - loss 0.0467\n",
            "TRAIN (epoch 5, batch 0) - accuracy 0.9594 - loss 0.0377 - grad norm 1.8626\n",
            "TRAIN (epoch 5, batch 100) - accuracy 0.9688 - loss 0.0418 - grad norm 1.8659\n",
            "TRAIN (epoch 5, batch 200) - accuracy 0.9703 - loss 0.0377 - grad norm 2.3378\n",
            "TRAIN (epoch 5, batch 300) - accuracy 0.9406 - loss 0.0487 - grad norm 1.4085\n",
            "VALID (epoch 5) - accuracy 0.9555 - loss 0.0455\n",
            "TRAIN (epoch 6, batch 0) - accuracy 0.9656 - loss 0.0346 - grad norm 1.5752\n",
            "TRAIN (epoch 6, batch 100) - accuracy 0.9703 - loss 0.0421 - grad norm 2.1450\n",
            "TRAIN (epoch 6, batch 200) - accuracy 0.9672 - loss 0.0367 - grad norm 2.2055\n",
            "TRAIN (epoch 6, batch 300) - accuracy 0.9563 - loss 0.0455 - grad norm 1.3127\n",
            "VALID (epoch 6) - accuracy 0.9552 - loss 0.0469\n",
            "TRAIN (epoch 7, batch 0) - accuracy 0.9656 - loss 0.0358 - grad norm 2.0169\n",
            "TRAIN (epoch 7, batch 100) - accuracy 0.9734 - loss 0.0412 - grad norm 1.9974\n",
            "TRAIN (epoch 7, batch 200) - accuracy 0.9578 - loss 0.0385 - grad norm 2.7148\n",
            "TRAIN (epoch 7, batch 300) - accuracy 0.9578 - loss 0.0432 - grad norm 1.2917\n",
            "VALID (epoch 7) - accuracy 0.9517 - loss 0.0475\n",
            "TRAIN (epoch 8, batch 0) - accuracy 0.9688 - loss 0.0344 - grad norm 1.9406\n",
            "TRAIN (epoch 8, batch 100) - accuracy 0.9844 - loss 0.0359 - grad norm 1.8657\n",
            "TRAIN (epoch 8, batch 200) - accuracy 0.9594 - loss 0.0423 - grad norm 3.3602\n",
            "TRAIN (epoch 8, batch 300) - accuracy 0.9500 - loss 0.0400 - grad norm 0.9557\n",
            "VALID (epoch 8) - accuracy 0.9537 - loss 0.0451\n",
            "TRAIN (epoch 9, batch 0) - accuracy 0.9688 - loss 0.0311 - grad norm 1.7324\n",
            "TRAIN (epoch 9, batch 100) - accuracy 0.9844 - loss 0.0316 - grad norm 1.7533\n",
            "TRAIN (epoch 9, batch 200) - accuracy 0.9656 - loss 0.0429 - grad norm 3.2912\n",
            "TRAIN (epoch 9, batch 300) - accuracy 0.9563 - loss 0.0404 - grad norm 1.1311\n",
            "VALID (epoch 9) - accuracy 0.9619 - loss 0.0410\n",
            "TRAIN (epoch 10, batch 0) - accuracy 0.9781 - loss 0.0281 - grad norm 1.3463\n",
            "TRAIN (epoch 10, batch 100) - accuracy 0.9766 - loss 0.0284 - grad norm 1.2210\n",
            "TRAIN (epoch 10, batch 200) - accuracy 0.9688 - loss 0.0405 - grad norm 2.6675\n",
            "TRAIN (epoch 10, batch 300) - accuracy 0.9563 - loss 0.0384 - grad norm 1.2415\n",
            "VALID (epoch 10) - accuracy 0.9657 - loss 0.0324\n",
            "TRAIN (epoch 11, batch 0) - accuracy 0.9797 - loss 0.0219 - grad norm 1.0614\n",
            "TRAIN (epoch 11, batch 100) - accuracy 1.0000 - loss 0.0047 - grad norm 0.7934\n",
            "TRAIN (epoch 11, batch 200) - accuracy 1.0000 - loss 0.0029 - grad norm 0.7299\n",
            "TRAIN (epoch 11, batch 300) - accuracy 1.0000 - loss 0.0030 - grad norm 0.6248\n",
            "VALID (epoch 11) - accuracy 0.9999 - loss 0.0026\n",
            "TRAIN (epoch 12, batch 0) - accuracy 1.0000 - loss 0.0022 - grad norm 0.9564\n",
            "TRAIN (epoch 12, batch 100) - accuracy 1.0000 - loss 0.0012 - grad norm 0.3858\n",
            "TRAIN (epoch 12, batch 200) - accuracy 1.0000 - loss 0.0028 - grad norm 0.8979\n",
            "TRAIN (epoch 12, batch 300) - accuracy 1.0000 - loss 0.0013 - grad norm 0.2368\n",
            "VALID (epoch 12) - accuracy 0.9999 - loss 0.0013\n",
            "TRAIN (epoch 13, batch 0) - accuracy 1.0000 - loss 0.0011 - grad norm 0.3530\n",
            "TRAIN (epoch 13, batch 100) - accuracy 1.0000 - loss 0.0068 - grad norm 2.0775\n",
            "TRAIN (epoch 13, batch 200) - accuracy 1.0000 - loss 0.0011 - grad norm 0.3702\n",
            "TRAIN (epoch 13, batch 300) - accuracy 1.0000 - loss 0.0025 - grad norm 0.5157\n",
            "VALID (epoch 13) - accuracy 0.9999 - loss 0.0034\n",
            "TRAIN (epoch 14, batch 0) - accuracy 1.0000 - loss 0.0031 - grad norm 1.1342\n",
            "TRAIN (epoch 14, batch 100) - accuracy 1.0000 - loss 0.0014 - grad norm 0.4610\n",
            "TRAIN (epoch 14, batch 200) - accuracy 1.0000 - loss 0.0014 - grad norm 0.6007\n",
            "TRAIN (epoch 14, batch 300) - accuracy 1.0000 - loss 0.0014 - grad norm 0.3810\n",
            "VALID (epoch 14) - accuracy 0.9999 - loss 0.0012\n",
            "TRAIN (epoch 15, batch 0) - accuracy 1.0000 - loss 0.0011 - grad norm 0.3708\n",
            "TRAIN (epoch 15, batch 100) - accuracy 1.0000 - loss 0.0044 - grad norm 1.3805\n",
            "TRAIN (epoch 15, batch 200) - accuracy 1.0000 - loss 0.0049 - grad norm 1.1705\n",
            "TRAIN (epoch 15, batch 300) - accuracy 1.0000 - loss 0.0063 - grad norm 1.3026\n",
            "VALID (epoch 15) - accuracy 0.9999 - loss 0.0024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Z5uQOK1OOQAf",
        "outputId": "d08b8eab-e77f-4908-8b01-1a185c71cc5e"
      },
      "source": [
        "acc, loss = test_eval(\"test_a8_seq20.txt\", c_hist, model, device)\n",
        "\n",
        "\"TEST acc {:.4f} - loss {:.4f}\".format(acc, loss)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TEST acc 1.0000 - loss 0.0025'"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D8STiB5h4-0"
      },
      "source": [
        "hooks, handles = add_hooks(model)\n",
        "\n",
        "seq = np.random.choice(np.arange(ALPHABET_SIZE), size=SEQ_LEN)\n",
        "\n",
        "model(torch.LongTensor([seq]).to(\"cuda\"))\n",
        "\n",
        "remove_hooks(handles)"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEXxF9FwldWw"
      },
      "source": [
        "attention_map = hooks[\"l0\"].saved_outs[0].squeeze(0)"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "wsl7DtTWlgBa",
        "outputId": "f16b508b-5930-4c5b-c0ec-6a90bba3c5c6"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "decoded_seq = [string.ascii_lowercase[id] for id in seq]\n",
        "\n",
        "ax = seaborn.heatmap(\n",
        "    attention_map, \n",
        "    square=True, \n",
        "    xticklabels=decoded_seq, \n",
        "    yticklabels=decoded_seq\n",
        ")\n",
        "\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaYqDW55lyJ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}