{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abstract__: Realisation of some ideas from the paper 'WebGPT: Browser-assisted question-answering with human feedback.' Here we are trying to leverage human feedback to enhance the summarization model. We finetune a modified model to score the answer and then integrate rejection sampling.\n",
    "\n",
    "Dataset: https://github.com/openai/summarize-from-feedback\n",
    "\n",
    "Model: sshleifer/distilbart-xsum-12-1 (https://huggingface.co/sshleifer/distilbart-xsum-12-1), pretrained on XSum and CNN_daylymain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xX5s3PiI6qLA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OyZwMkgt06OZ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-xsum-12-1\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-xsum-12-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CtZJF9Zt99Ss"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class GraderModel(nn.Module):\n",
    "    def __init__(self, model_max_len=512):\n",
    "        super(GraderModel, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-xsum-12-1\")\n",
    "\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-xsum-12-1\")\n",
    "        self.stack_layer = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(model_max_len * 50264, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        encoded_input = self.tokenizer.encode(x, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "        answer = self.model(encoded_input)\n",
    "\n",
    "        return self.stack_layer(answer.logits)\n",
    "\n",
    "grader_model = GraderModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iQJF3iim4Rrk"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "\n",
    "class ComparisonDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "        self.data = []\n",
    "\n",
    "        for file in files:\n",
    "            with open(file) as json_file:\n",
    "                lines = json_file.readlines()\n",
    "\n",
    "                for line in lines:\n",
    "                    sample = json.loads(line)\n",
    "                    self.data.append([sample['info']['post'], sample['summaries'][0]['text'], sample['summaries'][1]['text'], sample['choice']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ld0S76ZI6w8S",
    "outputId": "f0e552c5-c5f6-4214-e243-c679bbbaae5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# model = model.to(device)\n",
    "grader_model = grader_model.to(device)\n",
    "grader_model.model = grader_model.model.to(device)\n",
    "\n",
    "dataset = ComparisonDataset(['./data/batch4.json', './data/batch5.json', './data/batch9.json'])\n",
    "# dataset = ComparisonDataset(['/content/batch4.json'])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgA1O3noQ6Mv",
    "outputId": "7680253e-2889-4a01-8fd1-82ce82bca82a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, grader_model.parameters()))\n",
    "\n",
    "# num_epoch = 2 -> 0 cause using checkpoint\n",
    "num_epoch = 0\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in tqdm(range(num_epoch), position=0, leave=True):\n",
    "    loss_history.append([])\n",
    "\n",
    "    for reference, desc1, desc2, target in tqdm(dataloader, position=0, leave=True):\n",
    "        prompt1 = reference[0] + '|' + desc1[0]\n",
    "        prompt2 = reference[0] + '|' + desc2[0]\n",
    "\n",
    "        target = torch.tensor(target).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        score1 = grader_model(prompt1, device).squeeze()\n",
    "        score2 = grader_model(prompt2, device).squeeze()\n",
    "\n",
    "        comp = torch.stack([score1, score2]).unsqueeze(0)\n",
    "\n",
    "        loss = criterion(comp, target)\n",
    "        loss_history[-1].append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch: {}\\t|\\tLoss: {}'.format(epoch, np.mean(loss_history[-1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "obyhZFgYgGSg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_sampling(model, text, ans_n=3):\n",
    "    with torch.no_grad():\n",
    "        prompt = text\n",
    "        prompt = tokenizer.encode(prompt, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "        answers = model.generate(prompt, \n",
    "                                  do_sample=True,   \n",
    "                                  min_length=50, \n",
    "                                  max_length=768,\n",
    "                                  top_k=30,                                 \n",
    "                                  top_p=0.7,   \n",
    "                                  temperature=0.9,\n",
    "                                  repetition_penalty=2.0,\n",
    "                                  num_return_sequences=ans_n)\n",
    "        \n",
    "        ans = []\n",
    "\n",
    "        for answer in answers:\n",
    "            ans.append(tokenizer.decode(answer))\n",
    "\n",
    "        return ans\n",
    "        \n",
    "def rej_sampling(model, reward_model, text, ans_n=3):\n",
    "    with torch.no_grad():\n",
    "        prompt = text\n",
    "        prompt = tokenizer.encode(prompt, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "        answers = model.generate(prompt, \n",
    "                                  do_sample=True,   \n",
    "                                  min_length=50, \n",
    "                                  max_length=768,\n",
    "                                  top_k=30,                                 \n",
    "                                  top_p=0.7,   \n",
    "                                  temperature=0.9,\n",
    "                                  repetition_penalty=2.0,\n",
    "                                  num_return_sequences=15)\n",
    "        \n",
    "        rewards = []\n",
    "\n",
    "        for answer in answers:\n",
    "            prompt = text + '|' + tokenizer.decode(answer)\n",
    "\n",
    "            reward = reward_model(prompt, device)\n",
    "            rewards.append(reward.item())\n",
    "\n",
    "        idx = np.argsort(rewards)\n",
    "        ans = []\n",
    "        \n",
    "        for i in range(ans_n):\n",
    "            ans.append(tokenizer.decode(answers[idx[-i]])) \n",
    "\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(grader_model, f='kek')\n",
    "grader_model = torch.load('kek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ge0nAQF2d8yx",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programming\\conda\\lib\\site-packages\\transformers\\generation_utils.py:2343: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\n",
      "Simple abs:\n",
      "</s> The Eiffel Tower in Paris has become the tallest in the world, more than 60 years after it was built on the roof of the French capital's (19.5m) at the end of this week. \"9th Century\".</s>\n",
      "</s> The Eiffel Tower in Paris has become the tallest in the world, making its first official appearance since 1930s and 19th Century (20ft) was built in 20 years ago.\" as it was built on a tower in France.</s>\n",
      "</s> The Eiffel Tower in Paris has become the tallest in the world, making its first official appearance since 1930s and 20 years ago - it was built in the 1960s on 1 July. (5m) at a ceremony in Paris.</s>\n",
      "Rejection abs:\n",
      "</s> The Eiffel Tower in Paris has become the tallest in the world, making its first official appearance since 1930s and 19th Century (20ft) was built in 20 years ago.-7m) at a tower in Paris. </s><pad>\n",
      "</s> The Eiffel Tower in Paris has become the tallest in the world, more than 50 years after it was built on the roof of the French capital's (19.5m) at its highest height for nearly 60 years ago.\" and</s><pad>\n",
      "</s> The Eiffel Tower in Paris has officially opened its doors to the end of the year, with a three-storey tower in the 1960s and 20ft (7.5m) at the top of the French capital, Paris.</s><pad>\n",
      "Text: lagos, nigeria (cnn) a day after winning nigeria’s presidency, muhammadu buhari told cnn’s christiane amanpour that he plans to aggressively fight corruption that has long plagued nigeria and go after the root of the nation’s unrest. buhari said he’ll “rapidly give attention” to curbing violence in the northeast part of nigeria, where the terrorist group boko haram operates. by cooperating with neighboring nations chad, cameroon and niger, he said his administration is confident it will be able to thwart criminals and others contributing to nigeria’s instability. for the first time in nigeria’s history, the opposition defeated the ruling party in democratic elections. buhari defeated incumbent goodluck jonathan by about 2 million votes, according to nigeria’s independent national electoral commission. the win comes after a long history of military rule, coups and botched attempts at democracy in africa’s most populous nation.\n",
      "Simple abs:\n",
      "</s> Nigeria's new president has told the BBC that he plans to tackle violence and violence in the north-eastern state of Niger, a day after being elected. (1.5m) candidate for the country's first president.\" </s>\n",
      "</s> Nigeria's new president has told a BBC that he plans to tackle violence in the north-eastern state of nigeria, and said it will soon be able to help curb violence in recent years on. (Niger) after winning elections.</s>\n",
      "</s> Nigeria's new president has said he will not be able to tackle violence in the north-eastern country, after a long history of civil war and political turmoil on social media. - but says it is ready to take action against other countries.</s>\n",
      "Rejection abs:\n",
      "</s> Nigeria's new president has told the BBC that he plans to tackle violence and violence in the north-eastern state of Niger, a day after his election victory in last year's presidential election on Thursday night. (15:00 GMT.</s>\n",
      "</s> Nigeria's new president has said he plans to tackle violence and violence in the north-eastern country, after a long history of civil war on Sunday. (4m) win over the past two years ago. Â£1bn.</s>\n",
      "</s> Nigeria's new president has told the BBC that he plans to tackle violence and violence in the north-eastern country, after a long history of civil war on Sunday. (5m) victory over the nation's most populous nations across Africa.</s>\n",
      "Text: What kind of exercise do lazy people do? Diddly-squats\n",
      "Simple abs:\n",
      "</s> The BBC Sport looks at some of the best known exercise in the world, but how do you do not know what does it is a \"lack of exercise\" - and that's getting more exercise than they can't be done to work.</s><pad>\n",
      "</s> The BBC Sport website looks at some of the best exercise in the world, but it is not doing enough to do more exercise than a few days before they do you need to get exercise. has an extra exercise. Â£1m.</s><pad>\n",
      "</s> The BBC Sport website looks at some of the world's most lazy people do not exercise, but there is no more exercise than it has been a good idea for those who have to work in their own life-time training sessions. and will be on</s>\n",
      "Rejection abs:\n",
      "</s> The BBC Sport website looks at some of the world's most lazy people do not exercise, but what do you want to be doing more than a half-time exercise. and is an exercise in your own life.\" after getting stuck on.</s><pad>\n",
      "</s> The BBC Sport looks at some of the best exercise in the world, but it is not doing enough to do more exercise than just a few days before they get into training on Monday morning and 1 January 2016-18:00 BST (1.</s><pad>\n",
      "</s> The BBC Sport website looks at some of the world's best exercise - but it does not know how to do more exercise than a half a million people have to exercise, or even though they are doing so much work. in their own weight.</s><pad>\n",
      "Text: What is Forrest Gump's password? 1Forrest1.\n",
      "Simple abs:\n",
      "</s> What is the most famous film-maker Forrest Gump, but it does not know what happens when he's a password for his first time? to be found in the world of - and there will be one of the best known as Forrest.</s>\n",
      "</s> What is the most famous film of Forrest Gump, but there is no more than 1.5 million pounds (Â£1.3m) in a bid to find out what happens to be the best known as Forrest GUMP's?</s>\n",
      "</s> What is the most famous film of Forrest Gump, but there is no more than 1.2 million pounds (1.5m) in a bid to find out what happens when it comes to be an internet-to-date date?</s>\n",
      "Rejection abs:\n",
      "</s> What is the most famous film-maker Forrest Gump, but there is no sign that one of the world's best-known characters for his role.\" - and has been revealed in a series of questions about how do you can now?</s>\n",
      "</s> What is the most famous film of Forrest Gump, but there is no more than 1.5 million pounds (Â£1.6m) in a bid to keep him at the top of this year's - and what does it?</s>\n",
      "</s>What is the most famous film of Forrest Gump, but there are no longer than they can be used to write a password for the first time. Â£1.5m (2.6m) in one year's history.</s>\n",
      "Text: What do you call bears with no ears? B.\n",
      "Simple abs:\n",
      "</s> A bear that has no ears or even if you want to know what they call a \"bear with no ears\" - but there is one of the most famous bears in the world's best known as 'Bears' for... and it.</s>\n",
      "</s> A look at some of the world's most famous bear bears with no ears, but how do you know what they call a \"bear-like\" have been described as \"the biggest bear in this year\".. Â£1bn.\"</s>\n",
      "</s> A bear that has no ears or even if you want to know what they call a \"bear with no ears\" - but there is one of the most famous bears in the world's best-loved animals. Â£1bn?</s>\n",
      "Rejection abs:\n",
      "</s> A bear that has no ears or even if you want to know what they call a \"bear with no ears\" - but there is one of the most famous bears in the world's best known as 'Bears' for... and it.</s><pad><pad><pad>\n",
      "</s> A look at some of the best known bears with no ears, but how do you know what they call a bear that can be called \"bear with no ear\" - or even if you are not to get a bit too?. in...</s><pad><pad><pad>\n",
      "</s> A bear that has no ears in the US, but there is a lot of people who do not know what they call \"a bears with no ears\" - or even if they are to be called 'bears. Â£1.</s><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: What's a foot long and slippery? A slipper!\n",
      "Simple abs:\n",
      "</s> A foot-long has been found in the UK's biggest ever, but it can't take a step closer to the end of the year. and is now known as 'Slipper' or slippers - that they're going to be?</s>\n",
      "</s> A foot-long walker has been described as \"the world's most slippery\" - but it is not yet to be able to find out what happens when they're a slipper. Â£40,000 (6m) long</s>\n",
      "</s> A foot-long walker has been described as \"the world's most slippery\" in the US, but it is not too easy to find out how to get a slipper. and that they can't be used to make your feet.</s>\n",
      "Rejection abs:\n",
      "</s> A foot-long walker has been described as \" the world's most slippery\" - but how do you know what happens when they're going to be in a slipper. and is now known as 'Slipper' for'? of...</s><pad>\n",
      "</s> A foot, a foot-long walker has been described as \"the world's most slippery\" - but what happens when it comes to an end up of this year's on the World War Two? British National Lottery. Â©</s><pad>\n",
      "</s> A foot, a foot-long walker has been described as \"the world's most slippery\" - but what happens when it comes to an end up of this year's in the UK. Â£10m (1.5m).</s>\n",
      "Text: What are a shark's two most favorite words? Man overboard!\n",
      "Simple abs:\n",
      "</s> A shark has been spotted off the coast of a shark in the US, but there is no longer than any other people who want to be seen on board a shark-like \"man overboard\" at the end of this year's swimmer.</s>\n",
      "</s> A shark has been spotted off the coast of North America, in a bid to find out what's the best-known \" man overboard\" when he was caught by a shark on Sunday night and one of the world's most dangerous shark at.</s>\n",
      "</s> A shark has been caught up on a beach in the US, but there is one of the world's best-known \" man overboard\" after getting into trouble off the coast. Â£10m from an ocean at sea to shore.</s>\n",
      "Rejection abs:\n",
      "</s> A shark has been caught up on the deck of a shark, but there is no longer than anyone else in the world's best-known swimmer and to have his own life.\" after being pulled from off the coast at the sea.</s><pad>\n",
      "</s> A shark has been spotted off the coast of a shark in the US, but there is no longer than any other people who want to be seen on board a shark-like \"man overboard\" at the end of this year's swimmer.</s><pad>\n",
      "</s> A shark has been spotted off the coast of a shark in the US, but there is no longer than any other people who want to be seen on board a shark-like \" man overboard\" - and what's going to do you know.</s><pad>\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '''The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.''',\n",
    "    '''lagos, nigeria (cnn) a day after winning nigeria’s presidency, muhammadu buhari told cnn’s christiane amanpour that he plans to aggressively fight corruption that has long plagued nigeria and go after the root of the nation’s unrest. buhari said he’ll “rapidly give attention” to curbing violence in the northeast part of nigeria, where the terrorist group boko haram operates. by cooperating with neighboring nations chad, cameroon and niger, he said his administration is confident it will be able to thwart criminals and others contributing to nigeria’s instability. for the first time in nigeria’s history, the opposition defeated the ruling party in democratic elections. buhari defeated incumbent goodluck jonathan by about 2 million votes, according to nigeria’s independent national electoral commission. the win comes after a long history of military rule, coups and botched attempts at democracy in africa’s most populous nation.''',\n",
    "    '''What kind of exercise do lazy people do? Diddly-squats''',\n",
    "    '''What is Forrest Gump's password? 1Forrest1.''',\n",
    "    '''What do you call bears with no ears? B.''',\n",
    "    '''What's a foot long and slippery? A slipper!''',\n",
    "    '''What are a shark's two most favorite words? Man overboard!''',\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    simple_abs = simple_sampling(model, text)\n",
    "    rej_abs = rej_sampling(model, grader_model, text)\n",
    "\n",
    "    print('Text: {}\\nSimple abs:'.format(text))\n",
    "\n",
    "    for abs in simple_abs:\n",
    "        print(abs)\n",
    "\n",
    "    print('Rejection abs:')\n",
    "    \n",
    "    for abs in rej_abs:\n",
    "        print(abs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
