{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scarf_hacker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openml-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-5ggBqTEDjH",
        "outputId": "dc4339c4-c2a9-447a-a4b8-5190b94a9b29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openml-pytorch\n",
            "  Downloading openml-pytorch-0.0.5.tar.gz (18 kB)\n",
            "Collecting openml\n",
            "  Downloading openml-0.12.2.tar.gz (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from openml-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->openml-pytorch) (3.10.0.2)\n",
            "Collecting liac-arff>=2.4.0\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openml->openml-pytorch) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from openml->openml-pytorch) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from openml->openml-pytorch) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from openml->openml-pytorch) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from openml->openml-pytorch) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from openml->openml-pytorch) (1.19.5)\n",
            "Collecting minio\n",
            "  Downloading minio-7.1.3-py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from openml->openml-pytorch) (6.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openml->openml-pytorch) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->openml->openml-pytorch) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->openml->openml-pytorch) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->openml->openml-pytorch) (3.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from minio->openml->openml-pytorch) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from minio->openml->openml-pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openml->openml-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openml->openml-pytorch) (3.0.4)\n",
            "Building wheels for collected packages: openml-pytorch, openml, liac-arff\n",
            "  Building wheel for openml-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml-pytorch: filename=openml_pytorch-0.0.5-py3-none-any.whl size=18207 sha256=db916c78c73166901ceb7ecb4965f2f20a672f96920063ffd82f949a3188d77f\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/4a/9f/7bee098ce5076618f7ebc963f6cee2ca3e76e6ce967797cb31\n",
            "  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml: filename=openml-0.12.2-py3-none-any.whl size=137326 sha256=98802171ff498dc7cc11a01949d824094292d727dbfc2d2bd78def9dab8d6748\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/20/88/cf4ac86aa18e2cd647ed16ebe274a5dacee9d0075fa02af250\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11732 sha256=28c347ce583af6c2a690a2d520643210dfe3f2efb05c8b2608eedf1b55233ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
            "Successfully built openml-pytorch openml liac-arff\n",
            "Installing collected packages: xmltodict, minio, liac-arff, openml, openml-pytorch\n",
            "Successfully installed liac-arff-2.5.0 minio-7.1.3 openml-0.12.2 openml-pytorch-0.0.5 xmltodict-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt \n",
        "import time \n",
        "import numpy as np\n",
        "import random\n",
        "import openml\n",
        "import openml_pytorch\n",
        "import openml_pytorch.layers\n",
        "import openml_pytorch.config\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.stats import zscore\n",
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "\n",
        "# torch version \n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwzEwkBTEHaK",
        "outputId": "3c61273e-7c75-48aa-f944-b5c7661bec84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сама сеть\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(input_size, hidden_size))\n",
        "    layers.append(nn.ReLU())\n",
        "    for i in range(num_layers - 2):\n",
        "      layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "      layers.append(nn.ReLU())\n",
        "\n",
        "    layers.append(nn.Linear(hidden_size, output_size))\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "8sYIvyp9Ddwn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# получение и предобработка датасета\n",
        "def get_df_and_lables(num_task):\n",
        "  task = openml.datasets.get_dataset(num_task)\n",
        "  df = fetch_openml(name=task.name, as_frame=True)[\"data\"]\n",
        "  df = df.fillna(df.mean())\n",
        "  for column in df.columns:\n",
        "    df[column] = zscore(df[column])\n",
        "  labels = fetch_openml(name=task.name, as_frame=True)[\"target\"]\n",
        "  return df, labels"
      ],
      "metadata": {
        "id": "wCQsDvY3ExEw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, labels = get_df_and_lables(40983)"
      ],
      "metadata": {
        "id": "K6Xdyj7UHYtq"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Y79EfFnEIDr-",
        "outputId": "e7b8f3ca-8048-4109-dc8c-8bec5f7bd0fd"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-85151743-55aa-4838-bb29-4e5869dde0c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GLCM_Pan</th>\n",
              "      <th>Mean_G</th>\n",
              "      <th>Mean_R</th>\n",
              "      <th>Mean_NIR</th>\n",
              "      <th>SD_Plan</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.482765</td>\n",
              "      <td>-0.409632</td>\n",
              "      <td>0.049840</td>\n",
              "      <td>-0.697603</td>\n",
              "      <td>-0.354813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.157331</td>\n",
              "      <td>-0.452314</td>\n",
              "      <td>-0.015678</td>\n",
              "      <td>-1.095194</td>\n",
              "      <td>-0.724868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.582671</td>\n",
              "      <td>-0.507868</td>\n",
              "      <td>0.008900</td>\n",
              "      <td>-0.306223</td>\n",
              "      <td>-0.185094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.081103</td>\n",
              "      <td>-0.838533</td>\n",
              "      <td>-0.386084</td>\n",
              "      <td>-1.579725</td>\n",
              "      <td>-0.886132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.637662</td>\n",
              "      <td>-0.544001</td>\n",
              "      <td>-0.058305</td>\n",
              "      <td>0.045682</td>\n",
              "      <td>-0.641235</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85151743-55aa-4838-bb29-4e5869dde0c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85151743-55aa-4838-bb29-4e5869dde0c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85151743-55aa-4838-bb29-4e5869dde0c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   GLCM_Pan    Mean_G    Mean_R  Mean_NIR   SD_Plan\n",
              "0 -0.482765 -0.409632  0.049840 -0.697603 -0.354813\n",
              "1 -0.157331 -0.452314 -0.015678 -1.095194 -0.724868\n",
              "2  0.582671 -0.507868  0.008900 -0.306223 -0.185094\n",
              "3  0.081103 -0.838533 -0.386084 -1.579725 -0.886132\n",
              "4  0.637662 -0.544001 -0.058305  0.045682 -0.641235"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWhQi7bCIFZ7",
        "outputId": "1fc0a261-17ef-4d3c-e84f-4c502ee08609"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2\n",
              "1    2\n",
              "2    2\n",
              "3    2\n",
              "4    2\n",
              "Name: class, dtype: category\n",
              "Categories (2, object): ['1', '2']"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, df, target):\n",
        "        self.df = df\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.df.loc[idx, :]), int(self.target[idx]) - 1"
      ],
      "metadata": {
        "id": "bvd9qBg3bGxF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(*xs):\n",
        "    return [None if x is None else F.normalize(x, dim=-1) for x in xs]"
      ],
      "metadata": {
        "id": "caCm8k3mFsKE"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def pre_train(f, g, optimizer_f, optimizer_g, num_epochs, dataloaders):\n",
        "  best_model_wts = f.state_dict()\n",
        "  best_loss = Inf\n",
        "\n",
        "  since = time.time()\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "          f.train(True)\n",
        "          g.train(True) \n",
        "      else:\n",
        "          f.train(False)\n",
        "          g.train(False)\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for data in dataloaders[phase]:\n",
        "        inputs, labels = data\n",
        "        optimizer_f.zero_grad()\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        corrupted_batch = inputs.clone().numpy()\n",
        "        mask = random.sample([i for i in range(inputs.shape[1])], int(inputs.shape[1] * c))\n",
        "        for i in range(len(inputs)):\n",
        "          for j in range(len(mask)):\n",
        "            corrupted_batch[i][mask[j]] = random.choices(np.array(df.iloc[:, mask[j]]))[0]\n",
        "          \n",
        "        corrupted_batch = torch.tensor(corrupted_batch)\n",
        "        if use_gpu:\n",
        "          inputs = inputs.cuda()\n",
        "          labels = labels.cuda()\n",
        "          corrupted_batch = corrupted_batch.cuda()\n",
        "        z = g(f(inputs.float()))\n",
        "        z_fake = g(f(corrupted_batch.float()))\n",
        "        z, z_fake = normalize(z, z_fake)\n",
        "        logits = torch.matmul(z.T, z_fake)\n",
        "        labels = torch.arange(len(logits), device=\"cuda\" if use_gpu else \"cpu\")\n",
        "        loss = F.cross_entropy(logits, labels, reduction='sum')\n",
        "\n",
        "        if phase == 'train':\n",
        "          loss.backward()\n",
        "          optimizer_f.step()\n",
        "          optimizer_g.step()\n",
        "        running_loss += loss.item()\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "      print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
        "      if phase == 'val' and epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        best_model_wts = f.state_dict()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Elapsed {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  \n",
        "  return f, best_model_wts"
      ],
      "metadata": {
        "id": "sp5SDQ9Jdl0f"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "dataset= TabularDataset(df = df, target = labels)\n",
        "\n",
        "train_size = int(np.floor(0.7 * len(dataset)))\n",
        "val_size = int(np.floor(0.1 * len(dataset)))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
        "dataset_sizes = {\"train\": train_size, \"val\": val_size}"
      ],
      "metadata": {
        "id": "HZaF3FABIujP"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.numeric import Inf\n",
        "c = 0.6\n",
        "f = MLP(input_size = df.shape[1], hidden_size= 256, output_size = 256, num_layers=4)\n",
        "g = MLP(input_size = 256, hidden_size= 256, output_size = df.shape[1], num_layers=2)\n",
        "optimizer_f = optim.SGD(f.parameters(), lr=0.001)\n",
        "optimizer_g = optim.SGD(g.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "f, best_model_wts = pre_train(f, g, optimizer_f, optimizer_g, 20, dataloaders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dapAPavxrqjb",
        "outputId": "39ecc7f3-d82a-43b3-a649-c93023b000b8"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 0.0579\n",
            "val Loss: 0.0411\n",
            "Elapsed 0m 2s\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 0.0433\n",
            "val Loss: 0.0402\n",
            "Elapsed 0m 4s\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 0.0412\n",
            "val Loss: 0.0399\n",
            "Elapsed 0m 6s\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 0.0400\n",
            "val Loss: 0.0395\n",
            "Elapsed 0m 10s\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 0.0392\n",
            "val Loss: 0.0392\n",
            "Elapsed 0m 13s\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 0.0385\n",
            "val Loss: 0.0392\n",
            "Elapsed 0m 16s\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 0.0382\n",
            "val Loss: 0.0390\n",
            "Elapsed 0m 18s\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 0.0379\n",
            "val Loss: 0.0389\n",
            "Elapsed 0m 21s\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 0.0379\n",
            "val Loss: 0.0390\n",
            "Elapsed 0m 23s\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 0.0376\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 25s\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 0.0374\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 27s\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 0.0374\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 29s\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 0.0374\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 31s\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 0.0376\n",
            "val Loss: 0.0390\n",
            "Elapsed 0m 33s\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 0.0378\n",
            "val Loss: 0.0389\n",
            "Elapsed 0m 35s\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 0.0374\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 37s\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 0.0374\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 39s\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 0.0375\n",
            "val Loss: 0.0390\n",
            "Elapsed 0m 41s\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 0.0375\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 44s\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 0.0374\n",
            "val Loss: 0.0388\n",
            "Elapsed 0m 46s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(f, h, optimizer_f, optimizer_h, num_epochs, dataloaders):\n",
        "  best_acc = 0\n",
        "  since = time.time()\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "          f.train(True)\n",
        "          h.train(True) \n",
        "      else:\n",
        "          f.train(False)\n",
        "          h.train(False)\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      for data in dataloaders[phase]:\n",
        "        inputs, labels = data\n",
        "        optimizer_f.zero_grad()\n",
        "        optimizer_h.zero_grad()\n",
        "        if use_gpu:\n",
        "          inputs = inputs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        outputs = h(f(inputs.float()))\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "        # backward + optimize only if in training phase\n",
        "        if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer_f.step()\n",
        "            optimizer_h.step()\n",
        "        # statistics\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == labels).type(torch.float)\n",
        "\n",
        "    epoch_loss = running_loss / dataset_sizes[phase]\n",
        "    epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "        phase, epoch_loss, epoch_acc))\n",
        "    if phase == 'val' and epoch_acc > best_acc:\n",
        "      best_acc = epoch_acc\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "      time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))"
      ],
      "metadata": {
        "id": "wscB3_FHKt5j"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_f = optim.Adam(f.parameters(), lr=0.001)\n",
        "optimizer_h = optim.Adam(g.parameters(), lr=0.001)\n",
        "h = MLP(input_size = 256, hidden_size= 256, output_size = 2, num_layers=2)\n",
        "train(f, h, optimizer_f, optimizer_h, 10, dataloaders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuaj1X2Fty7y",
        "outputId": "ef0012a6-f945-4d99-8bde-3cac0eb51858"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "val Loss: 0.0013 Acc: 0.9607\n",
            "Epoch 1/9\n",
            "----------\n",
            "val Loss: 0.0010 Acc: 0.9607\n",
            "Epoch 2/9\n",
            "----------\n",
            "val Loss: 0.0007 Acc: 0.9607\n",
            "Epoch 3/9\n",
            "----------\n",
            "val Loss: 0.0004 Acc: 0.9793\n",
            "Epoch 4/9\n",
            "----------\n",
            "val Loss: 0.0003 Acc: 0.9834\n",
            "Epoch 5/9\n",
            "----------\n",
            "val Loss: 0.0004 Acc: 0.9834\n",
            "Epoch 6/9\n",
            "----------\n",
            "val Loss: 0.0003 Acc: 0.9834\n",
            "Epoch 7/9\n",
            "----------\n",
            "val Loss: 0.0004 Acc: 0.9814\n",
            "Epoch 8/9\n",
            "----------\n",
            "val Loss: 0.0003 Acc: 0.9834\n",
            "Epoch 9/9\n",
            "----------\n",
            "val Loss: 0.0003 Acc: 0.9855\n",
            "Training complete in 0m 12s\n",
            "Best val Acc: 0.985507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_corrects = 0\n",
        "for data in test_loader:\n",
        "  f.train(False)\n",
        "  h.train(False)\n",
        "  inputs, labels = data\n",
        "  outputs = h(f(inputs.float()))\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "  running_corrects += torch.sum(preds == labels).type(torch.float)\n",
        "\n",
        "acc = running_corrects.numpy() / test_size\n",
        "\n",
        "print(\"test accuracy =\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y82afCauPL9k",
        "outputId": "210c4534-1fc6-48c3-90d9-8f55cceec66b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy = 0.9876160990712074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = MLP(input_size = df.shape[1], hidden_size= 256, output_size = 256, num_layers=4)\n",
        "optimizer_f = optim.Adam(f.parameters(), lr=0.001)\n",
        "optimizer_h = optim.Adam(g.parameters(), lr=0.001)\n",
        "h = MLP(input_size = 256, hidden_size= 256, output_size = 2, num_layers=2)\n",
        "train(f, h, optimizer_f, optimizer_h, 10, dataloaders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41YV6zoXTBst",
        "outputId": "cc718c82-acbe-4161-d2b1-d600513b844a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "val Loss: 0.0015 Acc: 0.9482\n",
            "Epoch 1/9\n",
            "----------\n",
            "val Loss: 0.0013 Acc: 0.9482\n",
            "Epoch 2/9\n",
            "----------\n",
            "val Loss: 0.0009 Acc: 0.9482\n",
            "Epoch 3/9\n",
            "----------\n",
            "val Loss: 0.0006 Acc: 0.9772\n",
            "Epoch 4/9\n",
            "----------\n",
            "val Loss: 0.0005 Acc: 0.9876\n",
            "Epoch 5/9\n",
            "----------\n",
            "val Loss: 0.0005 Acc: 0.9876\n",
            "Epoch 6/9\n",
            "----------\n",
            "val Loss: 0.0003 Acc: 0.9896\n",
            "Epoch 7/9\n",
            "----------\n",
            "val Loss: 0.0003 Acc: 0.9896\n",
            "Epoch 8/9\n",
            "----------\n",
            "val Loss: 0.0004 Acc: 0.9855\n",
            "Epoch 9/9\n",
            "----------\n",
            "val Loss: 0.0002 Acc: 0.9896\n",
            "Training complete in 0m 11s\n",
            "Best val Acc: 0.989648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_corrects = 0\n",
        "for data in test_loader:\n",
        "  f.train(False)\n",
        "  h.train(False)\n",
        "  inputs, labels = data\n",
        "  outputs = h(f(inputs.float()))\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "  running_corrects += torch.sum(preds == labels).type(torch.float)\n",
        "\n",
        "acc = running_corrects.numpy() / test_size\n",
        "\n",
        "print(\"test accuracy =\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrDhoOM-TZwN",
        "outputId": "a69e97a3-abef-4592-ab21-259472f4fcb1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy = 0.9855521155830753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Новый датасет**"
      ],
      "metadata": {
        "id": "-Mn-u1riuJ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df, labels = get_df_and_lables(1053)"
      ],
      "metadata": {
        "id": "p_twkJU8Q1Uc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "7OStNcEsQsiz",
        "outputId": "83acd57e-e6f2-4897-f742-c25ed8bd9795"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dbc1400f-ef71-4ff8-acae-1ce2528a63c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loc</th>\n",
              "      <th>v(g)</th>\n",
              "      <th>ev(g)</th>\n",
              "      <th>iv(g)</th>\n",
              "      <th>n</th>\n",
              "      <th>v</th>\n",
              "      <th>l</th>\n",
              "      <th>d</th>\n",
              "      <th>i</th>\n",
              "      <th>e</th>\n",
              "      <th>b</th>\n",
              "      <th>t</th>\n",
              "      <th>lOCode</th>\n",
              "      <th>lOComment</th>\n",
              "      <th>lOBlank</th>\n",
              "      <th>locCodeAndComment</th>\n",
              "      <th>uniq_Op</th>\n",
              "      <th>uniq_Opnd</th>\n",
              "      <th>total_Op</th>\n",
              "      <th>total_Opnd</th>\n",
              "      <th>branchCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.534225</td>\n",
              "      <td>-0.380102</td>\n",
              "      <td>-0.295508</td>\n",
              "      <td>-0.285373</td>\n",
              "      <td>-0.453283</td>\n",
              "      <td>-0.346848</td>\n",
              "      <td>7.255076</td>\n",
              "      <td>-0.688290</td>\n",
              "      <td>-0.817612</td>\n",
              "      <td>-0.084805</td>\n",
              "      <td>1.663474</td>\n",
              "      <td>-0.084755</td>\n",
              "      <td>-0.406860</td>\n",
              "      <td>-0.081873</td>\n",
              "      <td>-0.263405</td>\n",
              "      <td>0.853939</td>\n",
              "      <td>-0.993538</td>\n",
              "      <td>-0.583329</td>\n",
              "      <td>-0.441735</td>\n",
              "      <td>-0.450430</td>\n",
              "      <td>-0.437880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.535530</td>\n",
              "      <td>-0.410826</td>\n",
              "      <td>-0.354578</td>\n",
              "      <td>-0.329250</td>\n",
              "      <td>-0.454485</td>\n",
              "      <td>-0.347003</td>\n",
              "      <td>5.386279</td>\n",
              "      <td>-0.704325</td>\n",
              "      <td>-0.826329</td>\n",
              "      <td>-0.084806</td>\n",
              "      <td>1.199350</td>\n",
              "      <td>-0.084767</td>\n",
              "      <td>-0.423636</td>\n",
              "      <td>-0.192883</td>\n",
              "      <td>-0.363730</td>\n",
              "      <td>0.329798</td>\n",
              "      <td>-1.013453</td>\n",
              "      <td>-0.590831</td>\n",
              "      <td>-0.443056</td>\n",
              "      <td>-0.452423</td>\n",
              "      <td>-0.455586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.391486</td>\n",
              "      <td>0.050035</td>\n",
              "      <td>-0.354578</td>\n",
              "      <td>0.219208</td>\n",
              "      <td>0.335124</td>\n",
              "      <td>0.237456</td>\n",
              "      <td>-0.531578</td>\n",
              "      <td>0.327797</td>\n",
              "      <td>0.767373</td>\n",
              "      <td>-0.031788</td>\n",
              "      <td>0.240159</td>\n",
              "      <td>-0.031789</td>\n",
              "      <td>0.415171</td>\n",
              "      <td>0.806207</td>\n",
              "      <td>0.338540</td>\n",
              "      <td>0.329798</td>\n",
              "      <td>0.579778</td>\n",
              "      <td>0.721971</td>\n",
              "      <td>0.289753</td>\n",
              "      <td>0.394830</td>\n",
              "      <td>0.075590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.932161</td>\n",
              "      <td>-0.257206</td>\n",
              "      <td>-0.354578</td>\n",
              "      <td>-0.109867</td>\n",
              "      <td>1.946407</td>\n",
              "      <td>1.895536</td>\n",
              "      <td>-0.469285</td>\n",
              "      <td>0.154084</td>\n",
              "      <td>6.550025</td>\n",
              "      <td>0.086029</td>\n",
              "      <td>1.895537</td>\n",
              "      <td>0.086028</td>\n",
              "      <td>1.723710</td>\n",
              "      <td>2.915398</td>\n",
              "      <td>2.345027</td>\n",
              "      <td>0.853939</td>\n",
              "      <td>0.579778</td>\n",
              "      <td>4.435326</td>\n",
              "      <td>1.722360</td>\n",
              "      <td>2.238852</td>\n",
              "      <td>-0.278527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.065494</td>\n",
              "      <td>-0.180396</td>\n",
              "      <td>-0.354578</td>\n",
              "      <td>-0.000175</td>\n",
              "      <td>0.046536</td>\n",
              "      <td>-0.038498</td>\n",
              "      <td>-0.469285</td>\n",
              "      <td>0.161032</td>\n",
              "      <td>0.157495</td>\n",
              "      <td>-0.061101</td>\n",
              "      <td>-0.038315</td>\n",
              "      <td>-0.061101</td>\n",
              "      <td>0.029320</td>\n",
              "      <td>-0.192883</td>\n",
              "      <td>0.137892</td>\n",
              "      <td>-0.194344</td>\n",
              "      <td>-0.017684</td>\n",
              "      <td>-0.028201</td>\n",
              "      <td>0.052085</td>\n",
              "      <td>0.035993</td>\n",
              "      <td>-0.189998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbc1400f-ef71-4ff8-acae-1ce2528a63c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dbc1400f-ef71-4ff8-acae-1ce2528a63c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dbc1400f-ef71-4ff8-acae-1ce2528a63c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        loc      v(g)     ev(g)  ...  total_Op  total_Opnd  branchCount\n",
              "0 -0.534225 -0.380102 -0.295508  ... -0.441735   -0.450430    -0.437880\n",
              "1 -0.535530 -0.410826 -0.354578  ... -0.443056   -0.452423    -0.455586\n",
              "2  0.391486  0.050035 -0.354578  ...  0.289753    0.394830     0.075590\n",
              "3  1.932161 -0.257206 -0.354578  ...  1.722360    2.238852    -0.278527\n",
              "4 -0.065494 -0.180396 -0.354578  ...  0.052085    0.035993    -0.189998\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = labels.replace({\"true\": 1, \"false\": 0})"
      ],
      "metadata": {
        "id": "uOvJ_64GOklp"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, df, target):\n",
        "        self.df = df\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.df.loc[idx, :]), int(self.target[idx])"
      ],
      "metadata": {
        "id": "zNYZllFcO0no"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "use_gpu = torch.cuda.is_available()\n",
        "dataset= TabularDataset(df = df, target = labels)\n",
        "\n",
        "train_size = int(np.floor(0.7 * len(dataset)))\n",
        "val_size = int(np.floor(0.1 * len(dataset)))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
        "dataset_sizes = {\"train\": train_size, \"val\": val_size}"
      ],
      "metadata": {
        "id": "lSiQmoLFO-r6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0.6\n",
        "t = 1\n",
        "f = MLP(input_size = df.shape[1], hidden_size= 256, output_size = 256, num_layers=4)\n",
        "g = MLP(input_size = 256, hidden_size= 256, output_size = df.shape[1], num_layers=2)\n",
        "optimizer_f = optim.Adam(f.parameters(), lr=0.001)\n",
        "optimizer_g = optim.Adam(g.parameters(), lr=0.001)\n",
        "\n",
        "f, best_model_wts = pre_train(f, g, optimizer_f, optimizer_g, 100, dataloaders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNio69-JPETa",
        "outputId": "9499eda4-57d2-4dc6-846e-c384ad049a2c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 0.3320\n",
            "val Loss: 0.2744\n",
            "Elapsed 0m 4s\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.2041\n",
            "val Loss: 0.2021\n",
            "Elapsed 0m 7s\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.1710\n",
            "val Loss: 0.1869\n",
            "Elapsed 0m 11s\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.1457\n",
            "val Loss: 0.1558\n",
            "Elapsed 0m 14s\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.1361\n",
            "val Loss: 0.1415\n",
            "Elapsed 0m 18s\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.1225\n",
            "val Loss: 0.1578\n",
            "Elapsed 0m 22s\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.1156\n",
            "val Loss: 0.1416\n",
            "Elapsed 0m 25s\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.1082\n",
            "val Loss: 0.1213\n",
            "Elapsed 0m 29s\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.1033\n",
            "val Loss: 0.1139\n",
            "Elapsed 0m 32s\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.0953\n",
            "val Loss: 0.1143\n",
            "Elapsed 0m 36s\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.0928\n",
            "val Loss: 0.1137\n",
            "Elapsed 0m 40s\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.0916\n",
            "val Loss: 0.1075\n",
            "Elapsed 0m 43s\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.0886\n",
            "val Loss: 0.1079\n",
            "Elapsed 0m 47s\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.0887\n",
            "val Loss: 0.1201\n",
            "Elapsed 0m 51s\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.0866\n",
            "val Loss: 0.1074\n",
            "Elapsed 0m 54s\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.0826\n",
            "val Loss: 0.1016\n",
            "Elapsed 0m 58s\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.0812\n",
            "val Loss: 0.1036\n",
            "Elapsed 1m 2s\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.0807\n",
            "val Loss: 0.1044\n",
            "Elapsed 1m 5s\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.0800\n",
            "val Loss: 0.1004\n",
            "Elapsed 1m 9s\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.0776\n",
            "val Loss: 0.0982\n",
            "Elapsed 1m 13s\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.0755\n",
            "val Loss: 0.0910\n",
            "Elapsed 1m 16s\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.0745\n",
            "val Loss: 0.0944\n",
            "Elapsed 1m 20s\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.0764\n",
            "val Loss: 0.1002\n",
            "Elapsed 1m 24s\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.0765\n",
            "val Loss: 0.0926\n",
            "Elapsed 1m 27s\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.0685\n",
            "val Loss: 0.0888\n",
            "Elapsed 1m 31s\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.0701\n",
            "val Loss: 0.0939\n",
            "Elapsed 1m 35s\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.0677\n",
            "val Loss: 0.0805\n",
            "Elapsed 1m 39s\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.0667\n",
            "val Loss: 0.0857\n",
            "Elapsed 1m 42s\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.0689\n",
            "val Loss: 0.0895\n",
            "Elapsed 1m 46s\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.0670\n",
            "val Loss: 0.0809\n",
            "Elapsed 1m 50s\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.0636\n",
            "val Loss: 0.0879\n",
            "Elapsed 1m 53s\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.0662\n",
            "val Loss: 0.0858\n",
            "Elapsed 1m 57s\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.0636\n",
            "val Loss: 0.0856\n",
            "Elapsed 2m 1s\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.0636\n",
            "val Loss: 0.0873\n",
            "Elapsed 2m 4s\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.0653\n",
            "val Loss: 0.0837\n",
            "Elapsed 2m 8s\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.0651\n",
            "val Loss: 0.0830\n",
            "Elapsed 2m 12s\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.0602\n",
            "val Loss: 0.0773\n",
            "Elapsed 2m 16s\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.0598\n",
            "val Loss: 0.0839\n",
            "Elapsed 2m 20s\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.0617\n",
            "val Loss: 0.0838\n",
            "Elapsed 2m 24s\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.0627\n",
            "val Loss: 0.0830\n",
            "Elapsed 2m 28s\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.0607\n",
            "val Loss: 0.0798\n",
            "Elapsed 2m 32s\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.0589\n",
            "val Loss: 0.0748\n",
            "Elapsed 2m 37s\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.0611\n",
            "val Loss: 0.0785\n",
            "Elapsed 2m 41s\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.0590\n",
            "val Loss: 0.0771\n",
            "Elapsed 2m 45s\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.0551\n",
            "val Loss: 0.0745\n",
            "Elapsed 2m 49s\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.0600\n",
            "val Loss: 0.0820\n",
            "Elapsed 2m 53s\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.0557\n",
            "val Loss: 0.0747\n",
            "Elapsed 2m 57s\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.0559\n",
            "val Loss: 0.0730\n",
            "Elapsed 3m 1s\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.0572\n",
            "val Loss: 0.0713\n",
            "Elapsed 3m 6s\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.0561\n",
            "val Loss: 0.0707\n",
            "Elapsed 3m 10s\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.0544\n",
            "val Loss: 0.0722\n",
            "Elapsed 3m 13s\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.0543\n",
            "val Loss: 0.0705\n",
            "Elapsed 3m 17s\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.0536\n",
            "val Loss: 0.0720\n",
            "Elapsed 3m 21s\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.0533\n",
            "val Loss: 0.0741\n",
            "Elapsed 3m 24s\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.0535\n",
            "val Loss: 0.0771\n",
            "Elapsed 3m 28s\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.0538\n",
            "val Loss: 0.0757\n",
            "Elapsed 3m 32s\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.0539\n",
            "val Loss: 0.0767\n",
            "Elapsed 3m 36s\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.0539\n",
            "val Loss: 0.0733\n",
            "Elapsed 3m 39s\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.0525\n",
            "val Loss: 0.0702\n",
            "Elapsed 3m 43s\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.0513\n",
            "val Loss: 0.0719\n",
            "Elapsed 3m 47s\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.0515\n",
            "val Loss: 0.0677\n",
            "Elapsed 3m 50s\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.0507\n",
            "val Loss: 0.0708\n",
            "Elapsed 3m 54s\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.0512\n",
            "val Loss: 0.0711\n",
            "Elapsed 3m 58s\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.0499\n",
            "val Loss: 0.0699\n",
            "Elapsed 4m 1s\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.0497\n",
            "val Loss: 0.0671\n",
            "Elapsed 4m 5s\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.0506\n",
            "val Loss: 0.0731\n",
            "Elapsed 4m 9s\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.0484\n",
            "val Loss: 0.0690\n",
            "Elapsed 4m 13s\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.0502\n",
            "val Loss: 0.0620\n",
            "Elapsed 4m 16s\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.0490\n",
            "val Loss: 0.0648\n",
            "Elapsed 4m 20s\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.0505\n",
            "val Loss: 0.0763\n",
            "Elapsed 4m 24s\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.0487\n",
            "val Loss: 0.0616\n",
            "Elapsed 4m 27s\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.0504\n",
            "val Loss: 0.0676\n",
            "Elapsed 4m 31s\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.0491\n",
            "val Loss: 0.0644\n",
            "Elapsed 4m 35s\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.0497\n",
            "val Loss: 0.0711\n",
            "Elapsed 4m 38s\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.0476\n",
            "val Loss: 0.0697\n",
            "Elapsed 4m 42s\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.0498\n",
            "val Loss: 0.0647\n",
            "Elapsed 4m 46s\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.0481\n",
            "val Loss: 0.0615\n",
            "Elapsed 4m 49s\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.0484\n",
            "val Loss: 0.0640\n",
            "Elapsed 4m 53s\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.0481\n",
            "val Loss: 0.0669\n",
            "Elapsed 4m 57s\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.0451\n",
            "val Loss: 0.0621\n",
            "Elapsed 5m 0s\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.0445\n",
            "val Loss: 0.0633\n",
            "Elapsed 5m 4s\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.0472\n",
            "val Loss: 0.0607\n",
            "Elapsed 5m 8s\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.0463\n",
            "val Loss: 0.0647\n",
            "Elapsed 5m 12s\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.0464\n",
            "val Loss: 0.0595\n",
            "Elapsed 5m 15s\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.0467\n",
            "val Loss: 0.0639\n",
            "Elapsed 5m 19s\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.0458\n",
            "val Loss: 0.0612\n",
            "Elapsed 5m 23s\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.0467\n",
            "val Loss: 0.0635\n",
            "Elapsed 5m 27s\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.0467\n",
            "val Loss: 0.0624\n",
            "Elapsed 5m 34s\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.0475\n",
            "val Loss: 0.0677\n",
            "Elapsed 5m 38s\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.0474\n",
            "val Loss: 0.0626\n",
            "Elapsed 5m 42s\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.0455\n",
            "val Loss: 0.0607\n",
            "Elapsed 5m 46s\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.0453\n",
            "val Loss: 0.0644\n",
            "Elapsed 5m 49s\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.0460\n",
            "val Loss: 0.0598\n",
            "Elapsed 5m 53s\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.0458\n",
            "val Loss: 0.0635\n",
            "Elapsed 5m 57s\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.0462\n",
            "val Loss: 0.0627\n",
            "Elapsed 6m 0s\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.0454\n",
            "val Loss: 0.0610\n",
            "Elapsed 6m 4s\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.0450\n",
            "val Loss: 0.0627\n",
            "Elapsed 6m 8s\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.0441\n",
            "val Loss: 0.0647\n",
            "Elapsed 6m 12s\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.0454\n",
            "val Loss: 0.0566\n",
            "Elapsed 6m 15s\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.0456\n",
            "val Loss: 0.0600\n",
            "Elapsed 6m 19s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0\n",
        "optimizer_f = optim.Adam(f.parameters(), lr=0.001)\n",
        "optimizer_h = optim.Adam(g.parameters(), lr=0.001)\n",
        "h = MLP(input_size = 256, hidden_size= 256, output_size = len(labels.unique()), num_layers=2)\n",
        "train(f, h, optimizer_f, optimizer_h, 20, dataloaders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1-Bk8hBRmAE",
        "outputId": "8450df99-ec75-4d6e-8266-f3b4179c62a1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8006\n",
            "Epoch 1/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8097\n",
            "Epoch 2/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8061\n",
            "Epoch 3/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8061\n",
            "Epoch 4/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8042\n",
            "Epoch 5/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8088\n",
            "Epoch 6/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8070\n",
            "Epoch 7/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8107\n",
            "Epoch 8/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8116\n",
            "Epoch 9/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8079\n",
            "Epoch 10/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8061\n",
            "Epoch 11/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8042\n",
            "Epoch 12/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8061\n",
            "Epoch 13/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8088\n",
            "Epoch 14/19\n",
            "----------\n",
            "val Loss: 0.0040 Acc: 0.8079\n",
            "Epoch 15/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8079\n",
            "Epoch 16/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8024\n",
            "Epoch 17/19\n",
            "----------\n",
            "val Loss: 0.0039 Acc: 0.8006\n",
            "Epoch 18/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8079\n",
            "Epoch 19/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8125\n",
            "Training complete in 0m 52s\n",
            "Best val Acc: 0.812500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_corrects = 0\n",
        "for data in test_loader:\n",
        "  f.train(False)\n",
        "  h.train(False)\n",
        "  inputs, labels = data\n",
        "  outputs = h(f(inputs.float()))\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "  running_corrects += torch.sum(preds == labels).type(torch.float)\n",
        "\n",
        "acc = running_corrects.numpy() / test_size\n",
        "\n",
        "print(\"test accuracy =\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0IwrG7nVo8R",
        "outputId": "6e27b85b-9654-4166-8180-294ff9fceb6d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy = 0.8112947658402204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = MLP(input_size = df.shape[1], hidden_size= 256, output_size = 256, num_layers=4)\n",
        "optimizer_f = optim.Adam(f.parameters(), lr=0.001)\n",
        "optimizer_h = optim.Adam(g.parameters(), lr=0.001)\n",
        "h = MLP(input_size = 256, hidden_size= 256, output_size = 2, num_layers=2)\n",
        "train(f, h, optimizer_f, optimizer_h, 20, dataloaders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4beB3TdVzBs",
        "outputId": "547efeec-57d2-4314-cdca-e4d35764c3bc"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8051\n",
            "Epoch 1/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8051\n",
            "Epoch 2/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8051\n",
            "Epoch 3/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8051\n",
            "Epoch 4/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8061\n",
            "Epoch 5/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8079\n",
            "Epoch 6/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8079\n",
            "Epoch 7/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8051\n",
            "Epoch 8/19\n",
            "----------\n",
            "val Loss: 0.0036 Acc: 0.8042\n",
            "Epoch 9/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8070\n",
            "Epoch 10/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8051\n",
            "Epoch 11/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8079\n",
            "Epoch 12/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8097\n",
            "Epoch 13/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8088\n",
            "Epoch 14/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8125\n",
            "Epoch 15/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8051\n",
            "Epoch 16/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8208\n",
            "Epoch 17/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8107\n",
            "Epoch 18/19\n",
            "----------\n",
            "val Loss: 0.0037 Acc: 0.8153\n",
            "Epoch 19/19\n",
            "----------\n",
            "val Loss: 0.0038 Acc: 0.8162\n",
            "Training complete in 0m 52s\n",
            "Best val Acc: 0.820772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_corrects = 0\n",
        "for data in test_loader:\n",
        "  f.train(False)\n",
        "  h.train(False)\n",
        "  inputs, labels = data\n",
        "  outputs = h(f(inputs.float()))\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "  running_corrects += torch.sum(preds == labels).type(torch.float)\n",
        "\n",
        "acc = running_corrects.numpy() / test_size\n",
        "\n",
        "print(\"test accuracy =\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am6zLfziWF9f",
        "outputId": "52268561-924f-48e0-ad50-f8f32d284eb6"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy = 0.8089990817263545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh7iA1NoTTQu",
        "outputId": "bcbef510-c37d-4f55-91fe-529becd3ad7b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-3.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.62.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.19.5)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.4.1)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.10.0.2)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABnet**"
      ],
      "metadata": {
        "id": "hfQ46LeJTzC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df, labels = get_df_and_lables(1053)\n",
        "x_train, x_test, y_train, y_test = train_test_split(df, labels, test_size=0.2)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.125)"
      ],
      "metadata": {
        "id": "qkFsZqgRTeij"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "\n",
        "clf = TabNetClassifier()\n",
        "clf.fit(\n",
        "  x_train.to_numpy(), y_train.to_numpy(),\n",
        "  eval_set=[(x_val.to_numpy(), y_val.to_numpy())]\n",
        ")\n",
        "preds = clf.predict(x_test.to_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTLCgJvxTVmB",
        "outputId": "33472b2b-295b-4826-c9d5-37d63bd6f854"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used : cpu\n",
            "epoch 0  | loss: 0.73328 | val_0_auc: 0.52752 |  0:00:00s\n",
            "epoch 1  | loss: 0.55522 | val_0_auc: 0.63843 |  0:00:00s\n",
            "epoch 2  | loss: 0.51281 | val_0_auc: 0.67008 |  0:00:01s\n",
            "epoch 3  | loss: 0.48332 | val_0_auc: 0.67762 |  0:00:01s\n",
            "epoch 4  | loss: 0.46382 | val_0_auc: 0.67196 |  0:00:02s\n",
            "epoch 5  | loss: 0.46514 | val_0_auc: 0.66598 |  0:00:02s\n",
            "epoch 6  | loss: 0.46671 | val_0_auc: 0.6535  |  0:00:02s\n",
            "epoch 7  | loss: 0.46548 | val_0_auc: 0.68028 |  0:00:03s\n",
            "epoch 8  | loss: 0.45969 | val_0_auc: 0.67721 |  0:00:03s\n",
            "epoch 9  | loss: 0.45876 | val_0_auc: 0.68215 |  0:00:04s\n",
            "epoch 10 | loss: 0.45917 | val_0_auc: 0.67423 |  0:00:04s\n",
            "epoch 11 | loss: 0.45835 | val_0_auc: 0.68116 |  0:00:04s\n",
            "epoch 12 | loss: 0.45554 | val_0_auc: 0.68113 |  0:00:05s\n",
            "epoch 13 | loss: 0.45415 | val_0_auc: 0.6841  |  0:00:05s\n",
            "epoch 14 | loss: 0.45816 | val_0_auc: 0.6838  |  0:00:06s\n",
            "epoch 15 | loss: 0.4536  | val_0_auc: 0.69162 |  0:00:06s\n",
            "epoch 16 | loss: 0.45216 | val_0_auc: 0.69377 |  0:00:06s\n",
            "epoch 17 | loss: 0.45317 | val_0_auc: 0.68778 |  0:00:07s\n",
            "epoch 18 | loss: 0.44968 | val_0_auc: 0.68712 |  0:00:07s\n",
            "epoch 19 | loss: 0.44852 | val_0_auc: 0.6881  |  0:00:08s\n",
            "epoch 20 | loss: 0.44703 | val_0_auc: 0.6915  |  0:00:08s\n",
            "epoch 21 | loss: 0.44872 | val_0_auc: 0.69365 |  0:00:08s\n",
            "epoch 22 | loss: 0.45043 | val_0_auc: 0.69601 |  0:00:09s\n",
            "epoch 23 | loss: 0.45005 | val_0_auc: 0.69965 |  0:00:09s\n",
            "epoch 24 | loss: 0.44801 | val_0_auc: 0.70163 |  0:00:09s\n",
            "epoch 25 | loss: 0.44762 | val_0_auc: 0.69917 |  0:00:10s\n",
            "epoch 26 | loss: 0.44639 | val_0_auc: 0.69934 |  0:00:10s\n",
            "epoch 27 | loss: 0.44595 | val_0_auc: 0.69489 |  0:00:11s\n",
            "epoch 28 | loss: 0.44895 | val_0_auc: 0.69859 |  0:00:11s\n",
            "epoch 29 | loss: 0.44626 | val_0_auc: 0.69664 |  0:00:11s\n",
            "epoch 30 | loss: 0.44466 | val_0_auc: 0.69951 |  0:00:12s\n",
            "epoch 31 | loss: 0.44644 | val_0_auc: 0.69721 |  0:00:12s\n",
            "epoch 32 | loss: 0.44556 | val_0_auc: 0.69833 |  0:00:13s\n",
            "epoch 33 | loss: 0.44464 | val_0_auc: 0.69934 |  0:00:13s\n",
            "epoch 34 | loss: 0.44479 | val_0_auc: 0.69377 |  0:00:13s\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.70163\n",
            "Best weights from best epoch are automatically used!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vfznIylVlu7",
        "outputId": "bb58bd72-3833-47cf-926b-72e879ec0de0"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8089113458888378"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    }
  ]
}